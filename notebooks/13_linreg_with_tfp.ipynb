{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNcCptvK1Sj9"
   },
   "source": [
    "# Modelling continuous data with Tensoflow Probability\n",
    "\n",
    "In this notebook you will learn how to work with TFP. You will set up linear regression models that are able to output a gaussian conditional probability distribution. You will define different models with Keras and the Tensorflow probability framework and optimize the negative log likelihood (NLL). You will model the conditional probability distribution as a Normal distribution with a constant, montonic and flexible standart deviation $\\sigma$. The mean $\\mu$ of the CPD will always depend linearly on the input. You compare the performace of the 3 models based on the NLL on a validaiton set and use the one with the lowest NLL to predict the test set. Finally, you will also do an extrapolation experiment with the best model and look how the predicted CPD behaves.\n",
    "\n",
    "\n",
    "**Dataset:** \n",
    "You work with a simulated dataset that looks a bit like a fish when visualized in a scatterplot. The data is simulated to have  a linear slope but non-constant variance. The variance starts with a large value then slowly decreases to increase again and in the end it decreases again. You split the data into train, validation and test dataset.\n",
    "\n",
    "**Content:**\n",
    "* Work with a Normal distribution in TFP\n",
    "* Simulate and split the dataset \n",
    "* Fit a model with keras and TFP that models the CPD with a linear mean $\\mu$ and a fixed standart deviation $\\sigma$\n",
    "* Fit a model with keras and TFP that models the CPD with a linear mean $\\mu$ and monotoic standart deviation $\\sigma$\n",
    "* Fit a model with keras and TFP that models the CPD with a linear mean $\\mu$ and flexible standart deviation $\\sigma$ (by adding hidden layers)\n",
    "* Compare the different models based on the NLL loss on the validation dataset\n",
    "* Choose one of these model to predict the test dataset and calculate the NLL on the test data\n",
    "* Make a extrapolation experiment with the best model and look how the predicted CPD behaves\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f9mojpXQ4Rj-"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t4IJDOa3Qkni"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 30 #make larger for better training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6_m-OXstXNm"
   },
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QllnEEUTEV63"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('default')\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "print(\"TFP Version\", tfp.__version__)\n",
    "print(\"TF  Version\",tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovc9_o17EV7T"
   },
   "source": [
    "### Working with a TFP Normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xc-8yC2F5vSU"
   },
   "source": [
    "Here you can see how to work with normal distribution in TFP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZL7xQjaDEV7W"
   },
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "d = tfd.Normal(loc=3, scale=1.5)           #A\n",
    "x = d.sample(2) # Draw two random points.    #B\n",
    "px = d.prob(x) # Compute density/mass.       #C\n",
    "print(x)\n",
    "print(px)\n",
    "#A create a 1D Normal distribution with mean 3 and standard deviation 1.5\n",
    "#B sample 2 realizations from the Normal distribution\n",
    "#C compute the likelihood for each of the two sampled values under the defined Normal distribution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yp71zexn30Cc"
   },
   "source": [
    "Further methods for TensorFlow Distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J9ETeIAH30Cg"
   },
   "outputs": [],
   "source": [
    "dist = tfd.Normal(loc=1.0, scale=0.1)\n",
    "print('sample   :', dist.sample(3).numpy()) #Samples 3 numbers\n",
    "print('prob     :',dist.prob((0,1,2)).numpy()) #Calculates the probabilities for positions 0,1,2\n",
    "print('log_prob :',dist.log_prob((0,1,2)).numpy()) #Same as above just log\n",
    "print('cdf      :',dist.cdf((0,1,2)).numpy()) #Calculates the cummulative distributions\n",
    "print('mean     :',dist.mean().numpy()) #Returns the mean of the distribution\n",
    "print('stddev   :',dist.stddev().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gnH_uHenLYtN"
   },
   "source": [
    "#### Exercise:\n",
    "Define a Normal distribution with mu = 5 and sigma = 1.2 and caluculate the probability density/mass function for the values 0, 0.1, 0.2, 0.3, ..., 10.  \n",
    "Do the same for a Poisson distribution with rate = 5 for the values 0, 1, 2, ..., 10 *Hint:  tfd.poisson.Poisson().*  \n",
    "Plot the results  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h9IxEOsuLX2f"
   },
   "outputs": [],
   "source": [
    "### Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LbagonrStPb8"
   },
   "source": [
    "### Simulate the fishlike data\n",
    "\n",
    "In the next few cells you will simulate some (x,y)-data where the y increases on average linerly with x but has non-constant variance that so that scatterplot looks like a fish.You will fist simulate random distributed noise with non constant variance, then uniformly distributed x values between -1 and 6 and finally calculate corresponding y values with y = 2.7*x+noise (linear slope of 2.7 and intercept of 0). The variance of the noise will change, it starts with a high value of 12 and gets smaller until it is reaches a constant value of 1, then it grows again until a value of 15 to stay constant for a while and to decrease to 1 again. Look at the plot to understand the behavior of the variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EGyWMIv6-PlO"
   },
   "outputs": [],
   "source": [
    "### define variance structure of the simulation\n",
    "x1=np.arange(1,12,0.1)\n",
    "x1=x1[::-1]\n",
    "x2=np.repeat(1,30)\n",
    "x3=np.arange(1,15,0.1)\n",
    "x4=np.repeat(15,50)\n",
    "x5=x3[::-1]\n",
    "x6=np.repeat(1,20)\n",
    "x=np.concatenate([x1,x2,x3,x4,x5,x6])\n",
    "plt.plot(x)\n",
    "plt.xlabel(\"index\",size=16)\n",
    "plt.ylabel(\"sigma\",size=16)#pred\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_wa7qrE_C-H"
   },
   "source": [
    "Now you will sample uniformly distributed x values in the range from -1 to 6. You will sample less x values in the range from -1 to 1. Finally you sort the x values (for ploting reasons). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "60b0dOUd9wKL"
   },
   "outputs": [],
   "source": [
    "# generation the x values for the simulated data\n",
    "np.random.seed(4710)\n",
    "noise=np.random.normal(0,x,len(x))\n",
    "np.random.seed(99)\n",
    "first_part=len(x1)\n",
    "x11=np.random.uniform(-1,1,first_part)\n",
    "np.random.seed(97)\n",
    "x12=np.random.uniform(1,6,len(noise)-first_part)\n",
    "x=np.concatenate([x11,x12])\n",
    "x=np.sort(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g81DrgF8CkuA"
   },
   "source": [
    "Let's put it all together to make the simulated fishlike data complete. You calculate y from the x values and the noise with a linear function where the slope is 2.7 and the intercept is 0, y=2.7*x+noise.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XDGPV4WX9wHU"
   },
   "outputs": [],
   "source": [
    "## generation the y values for the simulated noise and the x values\n",
    "y=2.7*x+noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rOZ-tdQ8YiQV"
   },
   "outputs": [],
   "source": [
    "y=y.reshape((len(y),1))\n",
    "x=x.reshape((len(x),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x2qConCfI1yQ"
   },
   "outputs": [],
   "source": [
    "# lets visualize the data\n",
    "plt.scatter(x,y,color=\"steelblue\") \n",
    "plt.xlabel(\"x\",size=16)\n",
    "plt.ylabel(\"y\",size=16)#pred\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pA9H9VStAzow"
   },
   "source": [
    "#### Split data in train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_U-vd1VSFLBt"
   },
   "source": [
    "In the next cells you will spilt the data x and y into a training, validation and test set. To get a first train and test dataset you just randomly sample 25% of the x and y values in the test dataset and the rest is the training dataset. The resulting training dataset gets splitted again into a training and validation dataset (80% training and 20% validation). After the splitting of the dataset you need to make sure that all the x values  from every dataset are in increasing order for ploting reasons (note that you also need to reorder the corresponding y values for all datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HNUefcVZqqg2"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=47)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=22)\n",
    "\n",
    "print(\"nr of traning samples = \",len(x_train))\n",
    "print(\"nr of validation samples = \",len(x_val))\n",
    "print(\"nr of test samples = \",len(x_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f-F99DjfxcAw"
   },
   "outputs": [],
   "source": [
    "## reordering so x values are in increasiong order\n",
    "order_idx_train=np.squeeze(x_train.argsort(axis=0))\n",
    "x_train=x_train[order_idx_train]\n",
    "y_train=y_train[order_idx_train]\n",
    "\n",
    "order_idx_val=np.squeeze(x_val.argsort(axis=0))\n",
    "x_val=x_val[order_idx_val]\n",
    "y_val=y_val[order_idx_val]\n",
    "\n",
    "order_idx_test=np.squeeze(x_test.argsort(axis=0))\n",
    "x_test=x_test[order_idx_test]\n",
    "y_test=y_test[order_idx_test]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QfKxdFYBH93m"
   },
   "source": [
    "Let's plot the training and validation data. You can see that it really looks a bit like a fish. In the following cells you will train different models on the training data, validate the loss (NLL) on the validation data and in the end you will predict the testdata with the best model. It's important to keep the testdata in a locked safe, because in practice it is unknown, until you decide which model you want to use to make a prediction. That is the reason why you will plot it only in the end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Aa9_Dwo5Nf-t"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(x_train,y_train,color=\"steelblue\")\n",
    "plt.xlabel(\"x\",size=16)\n",
    "plt.ylabel(\"y\",size=16)\n",
    "plt.title(\"train data\")\n",
    "plt.xlim([-1.5,6.5])\n",
    "plt.ylim([-30,55])\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(x_val,y_val,facecolors='none', edgecolors=\"steelblue\")\n",
    "plt.xlabel(\"x\",size=16)\n",
    "plt.ylabel(\"y\",size=16)\n",
    "plt.title(\"validation data\")\n",
    "plt.xlim([-1.5,6.5])\n",
    "plt.ylim([-30,55])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "13VLA4m_EV-D"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ib_fU9tcRz_X"
   },
   "source": [
    "## Fit a linear regression model with constant variance\n",
    "In the next cells you will define and fit a linear model on the simulated fish data with keras. You define a simple linear regression NN with only two parameters to model the output as a gaussian conditional probability distribution , for the loss we use the mse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "67Q3-VYLRyee"
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=(1,))\n",
    "params = Dense(1)(inputs) #B\n",
    "\n",
    "model_ = Model(inputs=inputs, outputs=params)\n",
    "model_.compile(Adam(learning_rate=0.01), loss=\"mse\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g5RDEDFNRybC"
   },
   "outputs": [],
   "source": [
    "model_.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M8KHL9TiSdDq"
   },
   "outputs": [],
   "source": [
    "history = model_.fit(x_train, y_train, epochs=EPOCHS, verbose=0, validation_data=(x_val,y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jg1NszifTmzE"
   },
   "source": [
    "To calculate the  minimal NLL we need to know the optimal constant sigma that minimizes the NLL. To get the optimal sigma we use the formulat for the optimal signma and compute a slightly adjusted standart deviation of the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ll0SobKoShwu"
   },
   "outputs": [],
   "source": [
    "preds_train = model_.predict(x_train)\n",
    "preds_val = model_.predict(x_val)\n",
    "\n",
    "SSR = np.sum(np.square(y_train-preds_train))\n",
    "sigma_= np.sqrt((SSR)/(len(x_train)-2))\n",
    "sigma_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xhKOx7r6S9Kz"
   },
   "source": [
    "To calculate the NLL with the optimal constant sigma, you can use the formula for the $$NLL = \\frac{1}{n}\\sum_{i=1}^{n}- log(\\frac{1}{\\sqrt{2 \\pi \\sigma^2_x}})+\\frac{(y_i - \\mu_i)^2}{2 \\sigma^2_x}$$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Az1VDhsWShtR"
   },
   "outputs": [],
   "source": [
    "#loss with the estimated sigma\n",
    "NLL_train=np.mean(-np.log(1/(np.sqrt(2*np.pi*np.square(sigma_))))+\n",
    "                  ((np.square(y_train-model_.predict(x_train))/(2*np.square(sigma_)))))\n",
    "print(NLL_train)\n",
    "NLL_val=np.mean(-np.log(1/(np.sqrt(2*np.pi*np.square(sigma_))))+\n",
    "                ((np.square(y_val-model_.predict(x_val))/(2*np.square(sigma_)))))\n",
    "print(NLL_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dILQYvQ5UOpY"
   },
   "source": [
    "Now you will plot the trained model with the resulting mean and +-2 sigma at each x value and see how well it fits the data. Remember in this model you used a linear model for the mean of the CPD and just fixed the standart deviation sigma to a constant value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "USmwR7bwUN36"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "\n",
    "x_pred = np.expand_dims(np.arange(-1,6,0.1), axis=1)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(x_train,y_train,color=\"steelblue\") #observerd \n",
    "preds = model_.predict(x_pred)\n",
    "sigma = sigma_\n",
    "plt.plot(x_pred,preds,color=\"black\",linewidth=2)\n",
    "plt.plot(x_pred,preds+2*sigma,color=\"red\",linestyle=\"--\",linewidth=2) \n",
    "plt.plot(x_pred,preds-2*sigma,color=\"red\",linestyle=\"--\",linewidth=2)\n",
    "plt.xlabel(\"x\",size=16)\n",
    "plt.ylabel(\"y\",size=16)\n",
    "plt.title(\"train data\")\n",
    "plt.xlim([-1.5,6.5])\n",
    "plt.ylim([-30,55])\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(x_val,y_val,color=\"steelblue\") #observerd \n",
    "plt.plot(x_pred,preds,color=\"black\",linewidth=2)\n",
    "plt.plot(x_pred,preds+2*sigma,color=\"red\",linestyle=\"--\",linewidth=2) \n",
    "plt.plot(x_pred,preds-2*sigma,color=\"red\",linestyle=\"--\",linewidth=2)\n",
    "plt.xlabel(\"x\",size=16)\n",
    "plt.ylabel(\"y\",size=16)\n",
    "plt.title(\"validation data\")\n",
    "plt.xlim([-1.5,6.5])\n",
    "plt.ylim([-30,55])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJgUkCIeo4y1"
   },
   "source": [
    "## Fit a linear regression model and allow the sd to depend in a monotonic way on the input\n",
    "\n",
    "Let's try to model the standart deviation sigma in a monotonic way. To define a monotonic standart deviation, you can use a Normal distribution in TFP and you fit both parameters the mean and sigma. You don't use any hidden layers in between. As loss function we use the NLL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V32TXJMHYBO5"
   },
   "outputs": [],
   "source": [
    "def NLL(y, distr):\n",
    "    return -distr.log_prob(y) #A\n",
    "\n",
    "def my_dist(params): \n",
    "    return tfd.Normal(loc = params[:,0:1], \n",
    "                    scale = 1e-3 + tf.math.softplus(0.05 * params[:,1:2])) # both parameters are learnable #C #D\n",
    "\n",
    "inputs = Input(shape=(1,))\n",
    "params = Dense(2)(inputs) #B\n",
    "dist = tfp.layers.DistributionLambda(my_dist)(params) #C #D\n",
    "\n",
    "model_monotoic_sd = Model(inputs=inputs, outputs=dist)\n",
    "model_monotoic_sd.compile(Adam(learning_rate=0.01), loss=NLL) \n",
    "\n",
    "#A Define NLL of the model \n",
    "#B Setting up the NN with two output node\n",
    "#C The first output node defines the mean (loc)\n",
    "#D The second output defines the standard deviation (scale) via the softplus function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ng9q8paln6i9"
   },
   "outputs": [],
   "source": [
    "model_monotoic_sd.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iRn1dI5Kn8n7"
   },
   "outputs": [],
   "source": [
    "history = model_monotoic_sd.fit(x_train, y_train, epochs=EPOCHS, verbose=0, validation_data=(x_val,y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPkN7OHU_-uy"
   },
   "source": [
    "**Not needed in novel versions of TF / TFP**\n",
    "\n",
    "In the next cell you define two models to predict the mean $\\mu$ and the standart deviation $\\sigma$ of the output distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iyM1HouzSvPp",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.ylabel('NLL')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhuzHofA30Hk"
   },
   "source": [
    "#### Result:  monotonic sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BIY1kiyDSvPx"
   },
   "outputs": [],
   "source": [
    "print(model_monotoic_sd.evaluate(x_train,y_train, verbose=0))\n",
    "print(model_monotoic_sd.evaluate(x_val,y_val, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XwwOBh0Aj4TM"
   },
   "outputs": [],
   "source": [
    "# The new API\n",
    "x_pred = np.arange(-1,6,0.1)\n",
    "model_monotoic_sd.predict(x_pred) # Creates samples\n",
    "preds = model_monotoic_sd(x_pred) # Returns **a distribution** for the x-values x_pred\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X6blTIYCICvX"
   },
   "outputs": [],
   "source": [
    "# From that distribution, we can calculate the means and sd\n",
    "preds_mu = preds.mean()\n",
    "preds_sd = preds.stddev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vgpvOOa_SvP5"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "\n",
    "x_pred = np.expand_dims(np.arange(-1,6,0.1), axis=1)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(x_train,y_train,color=\"steelblue\") #observerd \n",
    "\n",
    "plt.plot(x_pred,preds_mu,color=\"black\",linewidth=2)\n",
    "plt.plot(x_pred,preds_mu+2*preds_sd,color=\"red\",linestyle=\"--\",linewidth=2) \n",
    "plt.plot(x_pred,preds_mu-2*preds_sd,color=\"red\",linestyle=\"--\",linewidth=2)\n",
    "plt.xlabel(\"x\",size=16)\n",
    "plt.ylabel(\"y\",size=16)\n",
    "plt.title(\"train data\")\n",
    "plt.xlim([-1.5,6.5])\n",
    "plt.ylim([-30,55])\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(x_val,y_val,color=\"steelblue\") #observerd \n",
    "plt.plot(x_pred,preds_mu,color=\"black\",linewidth=2)\n",
    "plt.plot(x_pred,preds_mu+2*preds_sd,color=\"red\",linestyle=\"--\",linewidth=2) \n",
    "plt.plot(x_pred,preds_mu-2*preds_sd,color=\"red\",linestyle=\"--\",linewidth=2)\n",
    "plt.xlabel(\"x\",size=16)\n",
    "plt.ylabel(\"y\",size=16)\n",
    "plt.title(\"validation data\")\n",
    "plt.xlim([-1.5,6.5])\n",
    "plt.ylim([-30,55])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZvs9osI_tIR"
   },
   "source": [
    "Now that you have trained and ploted the model with the resulting mean and +-2  sigma at each x value you see how well it fits the data.You see that the standart deviation is not constant but increases with bigger x-values - this is still not describing the data variance very well. The mean of the distribution is still linear because there are no hidden layers in this model. Even if this model is more complex than the ones before, the NLL on the validation set with ~3.5 is unfortunately a bit higher compared to the model with the estimated constant sigma, which had a NLL of ~3.5. The main reason for this is because the simulated variance is not monotonic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wf63xZ_npMjJ"
   },
   "source": [
    "## Fit a linear regression and allow the sd to depend in a flexible manner on the input\n",
    "\n",
    "From the model with the monotonic standart deviation you probability guessed that you need a model where the standart deviation sigma is non-linear and able to increase for some x-values but then agian decrease for others. Remeber how the data was simulated. Let's try that and see if we can decrease the NLL below ~3.5. You will now, define and fit a model that models the mean of the normal distribution linearly (no hidden layer for the mean parameter) but for the standart devation sigma, you will use 3 hidden layers in beetween (with 50 ,100  and 50 nodes). Because of the hidden layers the resulting stadart deviation is now able to change non-linarly with x, if necessary. You will use again TFP and keras for the model defintion. The loss function you use again the NLL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sxiN_V87JkgX"
   },
   "outputs": [],
   "source": [
    "def NLL(y, distr):\n",
    "    return -distr.log_prob(y) \n",
    "\n",
    "def my_dist(params): \n",
    "    return tfd.Normal(loc = params[:,0:1], \n",
    "                    scale = 1e-3 + tf.math.softplus(0.05 * params[:,1:2]))# both parameters are learnable\n",
    "\n",
    "inputs = Input(shape=(1,))\n",
    "out1 = Dense(1)(inputs) #A\n",
    "hidden1 = Dense(50,activation=\"relu\")(inputs)\n",
    "hidden2 = Dense(100,activation=\"relu\")(hidden1)\n",
    "hidden3 = Dense(50,activation=\"relu\")(hidden2)\n",
    "out2 = Dense(1)(hidden3) #B\n",
    "params = Concatenate()([out1,out2]) #C\n",
    "dist = tfp.layers.DistributionLambda(my_dist)(params) \n",
    "\n",
    "model_flex_sd = Model(inputs=inputs, outputs=dist)\n",
    "model_flex_sd.compile(Adam(learning_rate=0.004), loss=NLL)\n",
    "#A The first output models the mean, no hidden layers are used\n",
    "#B The second output models the spread of the distribution. Three hidden layers are used for it\n",
    "#C Combining the outputs for the mean and the spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3D-GLvt8KFOR"
   },
   "outputs": [],
   "source": [
    "model_flex_sd.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cj5TqurzaM9E"
   },
   "outputs": [],
   "source": [
    "history = model_flex_sd.fit(x_train, y_train, epochs=EPOCHS, verbose=0, validation_data=(x_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4RHI2gCSaM9H"
   },
   "outputs": [],
   "source": [
    "#Not needed anymore\n",
    "#model_flex_sd_mean = Model(inputs=inputs, outputs=dist.mean())\n",
    "#model_flex_sd_sd = Model(inputs=inputs, outputs=dist.stddev())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MpXKrsYsaM9J"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.ylabel('NLL')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylim(2,6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xeJG0NGA30IR"
   },
   "source": [
    "#### Result: flexible sigma "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qEfpHXYAaMkp"
   },
   "outputs": [],
   "source": [
    "print(model_flex_sd.evaluate(x_train,y_train, verbose=0))\n",
    "print(model_flex_sd.evaluate(x_val,y_val, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y7XcEO67l6F2"
   },
   "outputs": [],
   "source": [
    "## The new API\n",
    "x_pred = np.arange(-1,6,0.1)\n",
    "preds = model_flex_sd(x_pred)\n",
    "preds_mu = preds.mean()\n",
    "preds_sd = preds.stddev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1n-IICA_aMhK"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(x_train,y_train,color=\"steelblue\") #observerd \n",
    "\n",
    "plt.plot(x_pred,preds_mu,color=\"black\",linewidth=2)\n",
    "plt.plot(x_pred,preds_mu+2*preds_sd,color=\"red\",linestyle=\"--\",linewidth=2) \n",
    "plt.plot(x_pred,preds_mu-2*preds_sd,color=\"red\",linestyle=\"--\",linewidth=2)\n",
    "plt.xlabel(\"x\",size=16)\n",
    "plt.ylabel(\"y\",size=16)\n",
    "plt.title(\"train data\")\n",
    "plt.xlim([-1.5,6.5])\n",
    "plt.ylim([-30,55])\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(x_val,y_val,color=\"steelblue\") #observerd \n",
    "plt.plot(x_pred,preds_mu,color=\"black\",linewidth=2)\n",
    "plt.plot(x_pred,preds_mu+2*preds_sd,color=\"red\",linestyle=\"--\",linewidth=2) \n",
    "plt.plot(x_pred,preds_mu-2*preds_sd,color=\"red\",linestyle=\"--\",linewidth=2)\n",
    "plt.xlabel(\"x\",size=16)\n",
    "plt.ylabel(\"y\",size=16)\n",
    "plt.title(\"validation data\")\n",
    "plt.xlim([-1.5,6.5])\n",
    "plt.ylim([-30,55])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXZTLZL4Kqj-"
   },
   "source": [
    "Now that you have trained and ploted the model with the resulting mean and +-2  sigma at each x value you see how well it fits the data. It looks really nice, just like a fish and exacly how we simulated the data. You see that the standart deviation is not constant but depends in a non-linear fashion on x. The mean still still depends linearly on x, constrained by the model, which has no hidden layer between x and the mean-output node. If you look at the NLL on the validation dataset you see that it really is lower than ~3.5, it now is only ~3.1. So with this more complex model you get the lowest NLL on the  the validation set!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ERqHpEWrakLt"
   },
   "source": [
    "### Prediction of the test with the best model\n",
    "#### Exercise:\n",
    "Which of the three fitted models shows the best prediction performance on new unseed data? Use the three fitted models to predict the test set and calculate the corresponding test NLL. What value do you expect for the test NLL? Does the observed test NLL meet your expectation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vnie7S7CNuwU"
   },
   "outputs": [],
   "source": [
    "# Write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMOSbFCCerrE"
   },
   "source": [
    "### Prediction outside the training x range\n",
    "\n",
    "#### Optinal Exercise: Do a little extrapolation experiment: Use the model that was trained on data with x-values between -1 and 6 to predicht the mean and sigma of the outcome y for a x-values in the range from -5 to 10. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7bkAHxttNwvk"
   },
   "outputs": [],
   "source": [
    "# Write your code here\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "13_linreg_with_tfp_new_version.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
