{"cells":[{"cell_type":"markdown","metadata":{"id":"zkwbsZYtY7_7"},"source":["## Banknote classification with fcNN without hidden layer compared to fcNN with hidden layer\n","\n","In this notebook you will do your first classification. You will see that fully connected networks without a hidden layer can only learn linar decision boundaries, while fully connected networks with hidden layers are able to learn non-linear decision boundaries.\n","\n","**Dataset:** You work with a banknote data set and classification task. We have 5 features of wavelet transformed images of banknotes:\n",">1. variance  (continuous feature)\n",">2. skewness (continuous feature)\n",">3. curtosis (continuous feature)\n",">4. entropy (continuous feature)\n",">5. class (binary indicating if the banknote is real or fake)  \n","\n","Don't worry too much about where these features come from.\n","\n","For this analysis we only use 2 features.\n","\n",">x1: skewness of wavelet transformed image  \n",">x2: entropy of wavelet transformed image\n","\n","\n","**The goal is to classify each banknote to either \"real\" (Y=0) or \"fake\" (Y=1).**\n","\n","\n","**Content:**\n","* visualize the data in a simple scatter plot and color the points by the class label\n","* use the Keras library to build a fcNN without hidden layers (logistic regression). Use SGD with the objective to minimize the crossentropy loss.\n","* visualize the learned decision boundary in a 2D plot\n","* use the Keras library to build a fcNN with a single hidden layer. Use SGD with the objective to minimize the crossentropy loss.\n","* visualize the learned decision boundary in a 2D plot\n","* compare the performace and the decision boundaries of the two models\n","* stack more hidden layers to the model and playaround with the epochs\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"DlIPKjGu-UEj"},"source":["#### Imports\n","\n","In the next two cells, we load all the required libraries and functions from keras and numpy. We also download the data with the 5 featues from the provided url."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TJSZXo7S-H5n"},"outputs":[],"source":["# load required libraries:\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","plt.style.use('default')\n","\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras import optimizers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Ofvh7fPYV9i"},"outputs":[],"source":["# Load data from url\n","from urllib.request import urlopen\n","url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt'\n","raw_data = urlopen(url)\n","dataset = np.loadtxt(raw_data, delimiter=\",\")\n","print(dataset.shape)"]},{"cell_type":"markdown","metadata":{"id":"HlDPWop1_zGM"},"source":["Let's extract the two featues *x1: skewness of wavelet transformed image* and *x2: entropy of wavelet transformed image*. We print the shape and see that for X  we have 1372 oberservations with two featues and for Y there are 1372 binary labels."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OHixTIf3fBFL"},"outputs":[],"source":["# Here we extract the two features and the labels of the dataset\n","X=dataset[:,[1,3]]\n","Y=dataset[:,4]\n","print(X.shape)\n","print(Y.shape)"]},{"cell_type":"markdown","metadata":{"id":"W2upXCjUBweV"},"source":["Since the banknotes are described by only 2 features, we can easily visualize the positions of real and fake banknotes in a 2D feature space. You can see that the boundary between the two classes is not separable by a straight line. A curved boundary line will do better. But even then we cannot expect a perfect seperation.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o91QiD0Ij8E8"},"outputs":[],"source":["# visualize the data in a 2D feature space.\n","idx_f = [np.where(Y==1)]\n","idx_r = [np.where(Y==0)]\n","plt.scatter(X[idx_r,0],X[idx_r,1], alpha=0.7,marker='^')\n","plt.scatter(X[idx_f,0],X[idx_f,1], alpha=0.7,marker='o')\n","plt.title(\"Real and fake banknotes\")\n","plt.xlabel(\"x1\")\n","plt.ylabel(\"x2\")\n","plt.legend((\"fake\",\"real\"),\n","           loc='lower left',\n","           fontsize=10)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"LXMTljrqtXqt"},"source":["### fcNN with only one neuron\n","Let’s try to use a single neuron with a sigmoid activation function (also known as logistic regression) as classification model to seperate the banknotes.  \n","We use the sequential API from keras to build the model. To fit the 3 parameters we use the stochastic gradient descent optimizer with a learning rate of 0.15."]},{"cell_type":"markdown","metadata":{"id":"64rucrOKe2mW"},"source":["#### Definition of a NN with only one neuron after the input\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wsfJWZi6gc04"},"outputs":[],"source":["# Definition of the network\n","model = Sequential()                                        # starts the definition of the network\n","model.add(Dense(1, batch_input_shape=(None, 2),             # adds a new layer to the network with a single neuron\n","                activation='sigmoid'))                      # The input is a tensor of size (batch_size, 2), since we don’t specify the Batch Size now, we use None as a placeholder\n","                                                            # chooses the activation function ‘sigmoid’\n","# Definition of the optimizer\n","sgd = optimizers.SGD(learning_rate=0.15)                               # Defining the stochastic gradient descent optimizer\n","\n","# compile model                                             # compile model, which ends the definition of the model\n","model.compile(loss='binary_crossentropy',\n","              optimizer=sgd,                                # using the stochastic gradient descent optimizer\n","              metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ZfkGjyjgneX"},"outputs":[],"source":["# summarize the architecture of the NN along with the number of weights\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"AI_YeSyBEHTc"},"source":["In the next cell, we train the network. In other words, we tune the parameters that were initialized randomly with stochastic gradient descent to minimize our loss function (the binary crossentropy). We set the batchsize to 128 per updatestep and train for 400 epochs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E34YcE-Wg34o"},"outputs":[],"source":["# Training of the network\n","history = model.fit(X, Y,                           # training of the model using the training data stored in X and Y for 4100 epochs\n","          epochs=400,                               # for 400 epochs\n","          batch_size=128,                           # fix the batch size to 128 examples\n","          verbose=0)\n"]},{"cell_type":"markdown","metadata":{"id":"zqE0FrJ9FPM2"},"source":["Let's look at the so called learning curve, we plot the accuracy and the loss vs the epochs. You can see that after 100 epochs, we predict around 70% of our data correct and have a loss around 0.51 (this values can vary from run to run)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vFQ5RgbciCqc"},"outputs":[],"source":["# plot the development of the accuracy and the loss during the training\n","plt.plot(history.history['accuracy'])\n","plt.title('model accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train'], loc='lower right')\n","plt.show()\n","plt.plot(history.history['loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train'], loc='upper right')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"aJxQApCTsSK1"},"source":["### Plotting the learned decision boundary\n","Let's visualize which decision boundary was learned by the fcNN with only one output neuron (and no hidden layer).  \n","As you can see the decision boundary is a straight line. This is not a coincidence but a general property of a single artificial neuron with a sigmoid as activation function and no hidden layer, also known as logistic regression.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DrWqT9T2oStj"},"outputs":[],"source":["def plotModel(X,Y, model, t):\n","    # define a grid for the 2D feature space\n","    # predict at each grid point the probability for class 1\n","\n","    x1list = np.linspace(np.min(X[:,0])-2, np.max(X[:,0])+2, 15) # Define 100 points on the x-axis\n","    x2list = np.linspace(np.min(X[:,1])-2, np.max(X[:,1])+2, 15) # Define 100 points on the x-axis\n","    X1_grid, X2_grid = np.meshgrid(x1list, x2list)\n","\n","    # model.predict for respective value x1 and x2\n","    p = np.array([model.predict(np.reshape(np.array([l1,l2]),(1,2))) for l1,l2 in zip(np.ravel(X1_grid), np.ravel(X2_grid))])\n","    print(p.shape)\n","    if len(p.shape) == 3 and p.shape[2]==2:\n","        p = p[:,:,1] # pick p for class 1 if there are more than 2 classes\n","    p = np.reshape(p,X1_grid.shape)\n","\n","    # visualize the predicted probabilities in the 2D feature space\n","    # once without and once with the data points used for fitting\n","    plt.figure(figsize=(16,4))\n","    plt.subplot(1,2,(1))\n","    cp = plt.contourf(X1_grid, X2_grid, p,cmap='RdBu_r')\n","    plt.colorbar(cp)\n","    plt.title(t)\n","    plt.xlabel('x1')\n","    plt.ylabel('x2')\n","\n","    plt.subplot(1,2,(2))\n","    cp = plt.contourf(X1_grid, X2_grid, p,cmap='RdBu_r')\n","    plt.colorbar(cp)\n","    idx_f = [np.where(Y==1)]\n","    idx_r = [np.where(Y==0)]\n","    plt.scatter(X[idx_r,0],X[idx_r,1], alpha=0.7,marker='^')\n","    plt.scatter(X[idx_f,0],X[idx_f,1], alpha=0.7,marker='o')\n","    plt.title(t)\n","    plt.xlabel('x1')\n","    plt.ylabel('x2')\n","\n","plotModel(X, Y, model, 'fcnn separation without hidden layer')"]},{"cell_type":"markdown","metadata":{"id":"-BVHMfQjr8LI"},"source":["### fcNN with one hidden layer\n","\n","We know that the boundary between the two classes is not descriped very good by a line. Therefore a single neuron is not appropriate to model the probability for a fake banknote based on its two features. To get a more flexible model, we introduce an additional layer between input layer and output layer. This is called hidden layer. Here we use a hidden layer with 8 neurons. We also change the ouputnodes from 1 to 2, to get two ouputs for the probability of real and fake banknote. Because we now have 2 outputs, we use the *softmax* activation function in the output layer. The softmax activation ensures that the output can be interpreted as a probability (see book for details)"]},{"cell_type":"markdown","metadata":{"id":"7Ar_CkKTfO3w"},"source":["#### Definition of the network with two hidden layers                                                                                                                     "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lgMpaj2hiHNX"},"outputs":[],"source":["# Definition of the network\n","model = Sequential()\n","model.add(Dense(8, batch_input_shape=(None, 2),activation='sigmoid'))\n","model.add(Dense(2, activation='softmax'))\n","\n","sgd = tf.keras.optimizers.legacy.SGD(learning_rate = 0.15)\n","\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=sgd,\n","              metrics=['accuracy'])"]},{"cell_type":"markdown","metadata":{"id":"aY3XX2qHOQQz"},"source":["In this is output summary we see that we now have a lot more trainable parameters than before.  \n","24 = inputdim · outputdim + outputbias= 2 · 8 + 8   \n","18 = inputdim · outputdim + outputbias= 8 · 2 + 2   "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QZ09IBXFidZQ"},"outputs":[],"source":["model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ztYP_-CIivRF"},"outputs":[],"source":["# Transforms Y=0 to (1,0) and Y=1 to (0,1)\n","Y_c=to_categorical(Y,2)\n","Y[0:5], Y_c[0:5]"]},{"cell_type":"markdown","metadata":{"id":"Soz6r8cGRAFh"},"source":["In the next cell, train the network. In other words, we tune the parameters that were initialized randomly with stochastic gradient descent to minimize our loss function (the categorical crossentropy). We set the batchsize to 128 per updatestep and train for 400 epochs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0fes2yosihh5"},"outputs":[],"source":["# Training of the network\n","history = model.fit(X, Y_c,\n","          epochs=400,\n","          batch_size=128,\n","          verbose=0)"]},{"cell_type":"markdown","metadata":{"id":"8-w1vdq-R0Wv"},"source":["Let's look again at the learning curve, we plot the accuracy and the loss vs the epochs. You can see that after 100 epochs, we predict around 86% of our data correct and have a loss aorund 0.29 (this values can vary from run to run). This is already a lot better than the model without a hidden layer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X1iTLPAoth6f"},"outputs":[],"source":["# plot the development of the accuracy and loss during the training\n","plt.plot(history.history['accuracy'])\n","plt.title('model accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train'], loc='lower right')\n","plt.show()\n","plt.plot(history.history['loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train'], loc='upper right')\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"YIoeyms8teNs"},"source":["### Plotting the learned decision boundary\n","Let's visualize which decision boundary was learned by the fcNN with the hidden layer\n","As you can see the decision boundary is a now curved and not straight anymore. The model (with the hidden layer in the middle) separates the the two classes in the training data better and is able to learn non-linear decision boundaries.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mg7PikB6sP2R"},"outputs":[],"source":["plotModel(X,Y,model, 'fcnn separation with hidden layer')"]},{"cell_type":"markdown","metadata":{"id":"tOX7awYjsWJb"},"source":["#### Add more hidden layers and play around with the training epochs\n","\n","Exercise: Add more hidden layers to the model and play around with the training epochs. What do you observe? Look at the learned decision boundary. How does the loss and the accuracy change?\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PdBT2MI-9wA6"},"outputs":[],"source":["### Your Code Here ###"]}],"metadata":{"colab":{"provenance":[]},"kernel_info":{"name":"python3"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.11"}},"nbformat":4,"nbformat_minor":0}