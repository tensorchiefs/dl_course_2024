{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "11_backpropagation",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bXDt1QFarxhc"
      },
      "source": [
        "## Performing a single backpropagation step to updata the parameter values once\n",
        "\n",
        "In this notebook you will see how to use tensorflow to do a single update step based on stochastic gradient descent with one data point. You will do one forward pass and one backward pass and extract the gradients of intermediate terms in the computational graph. You use them for computing the gradients of the loss w.r.t. the parameters (slope and intercept) which are needed to do one updatestep.\n",
        "\n",
        "**Dataset:** You work with a single datapoint of the systolic blood pressure and age data of 33 American women, which is generated in the upper part of the notebook . \n",
        "\n",
        "**Content:**\n",
        "* use the tensorflow library to set up the model \n",
        "    * define a computational graph containing all intermediate terms and local gradients \n",
        "    * do a single forward pass and compute all intermediate terms\n",
        "    * do a single backward pass and compute all local gradients and use them to compute the gradients of the loss w.r.t. the parameters via chain rule\n",
        "    * do a single update step of the parameter values\n",
        "    * verify that the computed values for the gradients and the updated parameter values are the same when you do it by hand\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wT-WLoi3XUyt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZkDvlnMjJxRe",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.style.use('default')\n",
        "import tensorflow as tf\n",
        "print('TF Version:', tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qIev26Cqa0VC"
      },
      "source": [
        "#### Blood Pressure data\n",
        "\n",
        "Here we read in the systolic blood pressure and the age of the 33 American women in our dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zACb9J35KP92",
        "colab": {}
      },
      "source": [
        "# Blood Pressure data\n",
        "x = [22, 41, 52, 23, 41, 54, 24, 46, 56, 27, 47, 57, 28, 48, 58,  9, \n",
        "     49, 59, 30, 49, 63, 32, 50, 67, 33, 51, 71, 35, 51, 77, 40, 51, 81]\n",
        "y = [131, 139, 128, 128, 171, 105, 116, 137, 145, 106, 111, 141, 114, \n",
        "     115, 153, 123, 133, 157, 117, 128, 155, 122, 183,\n",
        "     176,  99, 130, 172, 121, 133, 178, 147, 144, 217] \n",
        "x = np.asarray(x, np.float32) \n",
        "y = np.asarray(y, np.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4PVZDLZA0J3M"
      },
      "source": [
        "###  Doing the back propagation by hand for the example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GyWoQpLmjD_a"
      },
      "source": [
        "In the next cell we take only one woman of the dataset, because we want to calculate the gradients with only one datapoint. The woman is 58 years old and has a sbp value of 153."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uigQYLMs0B6s",
        "colab": {}
      },
      "source": [
        "x = x[14]\n",
        "y = y[14]\n",
        "print(x)\n",
        "print(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LLnZCN2wjYlL"
      },
      "source": [
        "Here we define the computational graph with all the intermediate values and gradients in between, because we need them to apply the the chain rule and do the backpropagation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q8thmhCF0fq9",
        "colab": {}
      },
      "source": [
        "# Defining the graph (construction phase)\n",
        "\n",
        "a_  = tf.Variable(0.0, name='a_var')                       # Variables, with starting values, will be optimized later\n",
        "b_  = tf.Variable(139.0, name='b_var')                     # we name them so that they look nicer in the graph\n",
        "x_  = tf.constant(x, name='x_const')                       # Constants, these are fixed tensors holding the data values and cannot be changed by the optimization\n",
        "y_  = tf.constant(y, name='y_const')  \n",
        "\n",
        "\n",
        "# We now do it step by step so that we can calculate the intermediate values and gradients\n",
        "def my_func():\n",
        "  ax_ = a_* x_\n",
        "  abx_ = ax_ + b_\n",
        "  r_ = abx_ - y_\n",
        "  s_ = tf.square(r_)\n",
        "  mse_ = tf.reduce_mean(s_)                                 \n",
        "  return([a_,b_,x_,y_,ax_,abx_,r_,s_,mse_])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jsNG3naRgPrU"
      },
      "source": [
        "#### Simple forward pass\n",
        "\n",
        "Now, let's do a simple forward pass and print the resulting values for ax, abx, r, s, and the mse."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egd5fDz6GJPd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a_,b_,x_,y_,ax_,abx_,r_,s_,mse_=my_func()\n",
        "vals = (ax_,abx_,r_,s_,mse_)\n",
        "vals"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tjnLQHR5gder"
      },
      "source": [
        "#### Extracting the gradients and the updated values\n",
        "\n",
        "In the next two cells we will extract all gradients of the graph in a backward pass and save all single gradients into a variable. We will also calculate the gradients of our tensorflow variables a and b w.r.t the loss (mean squared error) and do one update(\"apply_gradients\") of the slope and the intercept (we set the learning rate to 0.00002). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eejBz7HSHlbC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.SGD(0.00002)\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  ### get all single gradients of the backward pass\n",
        "  a_,b_,x_,y_,ax_,abx_,r_,s_,mse_=my_func()        \n",
        "  grad_mse_s = tape.gradient(mse_, s_)\n",
        "  grad_s_r = tape.gradient(s_, r_)\n",
        "  grad_r_abx_= tape.gradient(r_, abx_)\n",
        "  grad_abx_b = tape.gradient(abx_, b_)\n",
        "  grad_abx_ax = tape.gradient(abx_, ax_)\n",
        "  grad_ax_a = tape.gradient(ax_, a_)\n",
        "  ### get the gradients of a and b w.r.t the loss, here the mean squared error\n",
        "  gradients = tape.gradient(mse_, [a_,b_])\n",
        "  ### update the values of the slope a and the intercept b with the learning rate \n",
        "  optimizer.apply_gradients(zip(gradients,[a_,b_]))  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "reQ0PK5rvNRN",
        "colab": {}
      },
      "source": [
        "print(grad_mse_s.numpy(),grad_s_r.numpy(),grad_r_abx_.numpy(),grad_abx_b.numpy(),grad_abx_ax.numpy(),grad_ax_a.numpy())\n",
        "print(a_.numpy(),b_.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6H0Sl_CUqsE",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/tensorchiefs/dl_book/master/imgs/ch03_12.pdf.png\" width=\"800\" align=\"left\" />  \n",
        "Compare the results of tensorflow with the results form the lecture where we did the forward and the backward pass by hand. The forward pass in blue and the backward pass in red."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhRdf45yKCUH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a_  = tf.Variable(0.0, name='a_var')                       # Variables, with starting values, will be optimized later\n",
        "b_  = tf.Variable(139.0, name='b_var')                     # we name them so that they look nicer in the graph\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  a_,b_,x_,y_,ax_,abx_,r_,s_,mse_=my_func()        \n",
        "  grads_mse_a_b = tape.gradient(mse_, [a_,b_])\n",
        "grads_mse_a_b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-WWZE95oAvtD"
      },
      "source": [
        "#### Compute the gradient of the mse w.r.t to a via the chain rule "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tjLESXafnhQB",
        "colab": {}
      },
      "source": [
        "#grad_mse_a \n",
        "print(grad_mse_s.numpy()*grad_s_r.numpy()*grad_r_abx_.numpy()*grad_abx_ax.numpy()*grad_ax_a.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZT8eB1MlQWW2",
        "colab_type": "text"
      },
      "source": [
        "#### Compute the gradient of the mse w.r.t to b via the chain rule "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AUuVUqLwmg8Q",
        "colab": {}
      },
      "source": [
        "#grad_mse_b \n",
        "print(grad_mse_s.numpy()*grad_s_r.numpy()*grad_r_abx_.numpy()*grad_abx_b.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Subdq2jOmYMp"
      },
      "source": [
        "#### Update Formula\n",
        "Verify that we get the same if we do the upate \"by hand\".\n",
        "\n",
        "\n",
        "a_new=a_old - learning_rate * grad_mse_a  \n",
        "b_new=b_old - learning_rate * grad_mse_b   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yGa4Z599kwjx",
        "colab": {}
      },
      "source": [
        "a0=0\n",
        "b0=139\n",
        "eta=0.00002\n",
        "print(a0-eta*grads_mse_a_b[0].numpy())\n",
        "print(b0-eta*grads_mse_a_b[1].numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}