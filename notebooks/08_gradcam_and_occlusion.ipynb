{"cells":[{"cell_type":"markdown","metadata":{"id":"wYXe4o9cnwBg"},"source":["# 2D GradCam & 2D Occlusion  \n","In this notebook we are going to see how the Gradcam and an Occlusion Algorithm works.\n","We won't train a model by ourself but rather use the power of freely available models which are pretrained by others on a large set of images (Imagenet\n"," 14'197'122 images https://www.image-net.org/about.php). This notebook consist of the following steps:\n","\n","\n","1.   Download & set the weights to a given modelarchitecture\n","2.   Predict the class of an image with the model (likelihoods)\n","3.   xAI, inspect which region of the image had the highest impact on the prediction with:\n","  - GradCAM ++\n","  - Occlusion for GradCAM++\n","\n","\n","\n","\n","This notebook is adapted from https://keras.io/examples/vision/grad_cam/.\n","\n"]},{"cell_type":"markdown","source":["For the algorithms to work, we need some helper functions, if you are curious try to understand what they do, otherwise you can safely execute and  ignore the  cell below."],"metadata":{"id":"DWYm3gqAl51b"}},{"cell_type":"code","source":["# @title imports & helperfunctions\n","\n","# Tensor Manipulation\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from skimage.transform import resize\n","from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n","\n","# Display\n","from IPython.display import Image, display\n","import matplotlib.cm as cm\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","import ipywidgets as widgets\n","from tabulate import tabulate\n","\n","#IO\n","from google.colab import files\n","import os\n","import requests\n","import ast\n","\n","\n","def get_img_array(img_path, size):\n","    # `img` is a PIL image of size 299x299\n","    img = keras.preprocessing.image.load_img(img_path, target_size=size)\n","    # `array` is a float32 Numpy array of shape (size, 3)\n","    array = keras.preprocessing.image.img_to_array(img)\n","    # We add a dimension to transform our array into a \"batch\"\n","    # of size (1, size, 3)\n","    array = np.expand_dims(array, axis=0)\n","    return array\n","\n","\n","def make_gradcam_heatmap(img_array, model_2d, last_conv_layer_name, pred_index=None):\n","    # First, we create a model that maps the input image to the activations\n","    # of the last conv layer as well as the output predictions\n","    grad_model = tf.keras.models.Model(\n","        [model_2d.inputs], [model_2d.get_layer(last_conv_layer_name).output, model_2d.output]\n","    )\n","\n","    # Then, we compute the gradient of the top predicted class for our input image\n","    # with respect to the activations of the last conv layer\n","    with tf.GradientTape() as tape:\n","        last_conv_layer_output, preds = grad_model(img_array)\n","        if pred_index is None:\n","            pred_index = tf.argmax(preds[0])\n","        class_channel = preds[:, pred_index]\n","\n","    # This is the gradient of the output neuron (top predicted or chosen)\n","    # with regard to the output feature map of the last conv layer\n","    grads = tape.gradient(class_channel, last_conv_layer_output)\n","\n","    # This is a vector where each entry is the mean intensity of the gradient\n","    # over a specific feature map channel\n","    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n","\n","    # We multiply each channel in the feature map array\n","    # by \"how important this channel is\" with regard to the top predicted class\n","    # then sum all the channels to obtain the heatmap class activation\n","    last_conv_layer_output = last_conv_layer_output[0]\n","    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n","    heatmap = tf.squeeze(heatmap)\n","\n","    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n","    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n","\n","    # resize the heatmap to be the same size as the original image\n","    heatmap = heatmap.numpy()\n","    heatmap = resize(heatmap, img_array.shape[1:3])\n","    heatmap = (tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)).numpy()\n","\n","    return heatmap\n","\n","\n","def iter_occlusion(volume, size=4, stride = None):\n","    # volume: np array in shape 128, 128, 64, 1\n","\n","    occlusion_center = np.full((size, size, 3), [0], np.float32)\n","\n","    for y in range(0, volume.shape[0]-size+1, stride):\n","        for x in range(0, volume.shape[1]-size+1, stride):\n","            tmp = volume.copy()\n","\n","            tmp[y:y + size, x:x + size, :] = occlusion_center\n","\n","            yield x, y, tmp\n","\n","\n","def rgb_image_occlusion(volume, model_2d, correct_class, occlusion_size, occlusion_stride=None,\n","                       clip_pred_hm=True):\n","\n","    print('occluding...')\n","    if occlusion_stride is None:\n","        stride = occlusion_size\n","    elif occlusion_stride > occlusion_size:\n","        raise ValueError('stride must be smaller or equal size')\n","\n","    if occlusion_stride == occlusion_size:\n","        if (not (volume.shape[0] / occlusion_size).is_integer() or\n","                not (volume.shape[1] / occlusion_size).is_integer()):\n","\n","            raise ValueError('size does not work with this volume')\n","    elif occlusion_stride != occlusion_size:\n","        if (((volume.shape[0] - occlusion_size) % occlusion_stride) != 0 or\n","                ((volume.shape[1] - occlusion_size) % occlusion_stride) != 0):\n","\n","            raise ValueError('shape and size do not match')\n","    heatmap_prob_sum = np.zeros((volume.shape[0], volume.shape[1]), np.float32)\n","    heatmap_occ_n = np.zeros((volume.shape[0], volume.shape[1]), np.float32)\n","    total_steps = int(np.prod(((np.array(volume.shape[0:2]) - occlusion_size) / occlusion_stride) + 1))\n","    with tqdm(total=total_steps, desc=\"Calculating heatmap\") as pbar:\n","        for n, (x, y, vol_float) in enumerate(iter_occlusion(volume, size=occlusion_size, stride=occlusion_stride)):\n","            X = vol_float.reshape(1, volume.shape[0], volume.shape[1], volume.shape[2])\n","            out = model_2d.predict(X, verbose=0)\n","\n","            heatmap_prob_sum[y:y + occlusion_size, x:x + occlusion_size] += out[0, correct_class]\n","            heatmap_occ_n[y:y + occlusion_size, x:x + occlusion_size] += 1\n","            pbar.update(1)\n","    print(\"calculating heatmap...\")\n","    heatmap = heatmap_prob_sum / heatmap_occ_n\n","    if clip_pred_hm:\n","        cut_off = model_2d.predict(volume.reshape(1, volume.shape[0], volume.shape[1], volume.shape[2]), verbose=0)[0,\n","                                 correct_class]\n","        heatmap = np.abs(np.minimum(heatmap - cut_off, 0))\n","    return heatmap  # , class_pixels\n","\n","\n","def generate_all_images(img_array, heatmap, modprob):\n","    f, axs = plt.subplots(1, 3, figsize=(24,8))\n","    (ax1, ax2, ax3) = axs\n","\n","    ax1.imshow(img_array[0]/255, vmin = 0, vmax = 1)\n","    ax2.imshow(img_array[0]/255, vmin = 0, vmax = 1)\n","    ax2.imshow(heatmap, cmap='jet', vmin = np.min(heatmap), vmax = np.max(heatmap), alpha=0.4)\n","    im = ax3.imshow(heatmap, cmap='jet', vmin = np.min(heatmap), vmax = np.max(heatmap), alpha=0.4)\n","    cb = f.colorbar(im, ax=axs.ravel().tolist())\n","    cb.ax.axhline(y=modprob, linewidth = 3, c='black')\n","\n","\n","def generate_superimposed_image(img_array, heatmap, save_name = None):\n","    f, axs = plt.subplots(1, 1, figsize=(5,5))\n","    (ax1) = axs\n","\n","    ax1.imshow(img_array[0]/255, vmin = 0, vmax = 1)\n","    ax1.imshow(heatmap, cmap='jet', vmin = np.min(heatmap), vmax = np.max(heatmap), alpha=0.4)\n","\n","    plt.axis('off')\n","\n","    axins = inset_axes(\n","            ax1,\n","            width=\"5%\",  # width: 5% of parent_bbox width\n","            height=\"100%\",  # height: 50%\n","            loc=\"lower left\",\n","            bbox_to_anchor=(1.01, 0., 1, 1),\n","            bbox_transform=ax1.transAxes,\n","            borderpad=0,\n","        )\n","    plt.colorbar(\n","            matplotlib.cm.ScalarMappable(\n","                norm=matplotlib.colors.Normalize(vmin=heatmap.min(), vmax=heatmap.max(), clip=False),\n","                cmap=\"jet\"),\n","            cax=axins,\n","            label='',\n","            ticks=np.trunc(np.linspace(heatmap.min(), heatmap.max(), 5)*100)/100)\n","\n","    if save_name is not None:\n","        plt.savefig(save_name, bbox_inches='tight', dpi=300)\n","\n","\n","def upload_image_colab(filename):\n","    uploaded = files.upload()\n","    for uploaded_filename in uploaded.keys():\n","        os.rename(uploaded_filename, filename)\n","        display(Image(filename))\n","\n","\n","def set_model_globals(model_name):\n","    global model, img_size, preprocess_input, decode_predictions\n","    if model_name == 'VGG16':\n","        model_builder = keras.applications.vgg16.VGG16\n","        img_size = (224, 224)\n","        preprocess_input = keras.applications.vgg16.preprocess_input\n","        decode_predictions = keras.applications.vgg16.decode_predictions\n","    elif model_name == 'ResNet50V2':\n","        model_builder = keras.applications.resnet_v2.ResNet50V2\n","        img_size = (224, 224)\n","        preprocess_input = keras.applications.resnet_v2.preprocess_input\n","        decode_predictions = keras.applications.resnet_v2.decode_predictions\n","    elif model_name == 'Xception':\n","        model_builder = keras.applications.xception.Xception\n","        img_size = (299, 299)\n","        preprocess_input = keras.applications.xception.preprocess_input\n","        decode_predictions = keras.applications.xception.decode_predictions\n","    else:\n","        print(\"Invalid model selected\")\n","        return\n","    model = model_builder(weights=\"imagenet\")\n","\n","#widgets\n","model_dropdown = widgets.Dropdown(\n","    options=['VGG16', 'ResNet50V2', 'Xception'],\n","    description='Model:',\n",")\n","ok_button = widgets.Button(description=\"OK\")\n","\n","\n","def on_ok_button_clicked(b):\n","    set_model_globals(model_dropdown.value)\n","    if model is not None:\n","        print(f\"Selected Model: {model_dropdown.value}\")\n","        print(f\"Image size: {img_size}\")\n","        model.summary()\n","    else:\n","        print(\"No model selected or an error occurred.\")\n","\n","#button link\n","ok_button.on_click(on_ok_button_clicked)\n","\n","def load_labels(verbose=0):\n","  url = 'https://raw.githubusercontent.com/tensorchiefs/dl_course_2024/main/notebooks/imagenet_labels.txt'\n","  response = requests.get(url)\n","  if response.status_code == 200:\n","      content = response.text\n","      try:\n","          # Use ast.literal_eval to safely parse the dictionary string\n","          imagenet_classes = ast.literal_eval(content)\n","          print(\"Dictionary loaded successfully!\")\n","          if verbose ==1:\n","            for key in list(imagenet_classes.keys())[:5]:\n","                print(f\"{key}: {imagenet_classes[key]}\")\n","      except ValueError as e:\n","          print(f\"Error parsing the content: {e}\")\n","  display_options = [f\"{index}: {name}\" for index, name in imagenet_classes.items()]\n","  return display_options\n","\n","display_options = load_labels(verbose=0)\n","class_dropdown = widgets.Dropdown(options=display_options,description='ImageNet Class:',)\n","\n","# patch predictions\n","def patch_predict_image(x_range, y_range):\n","    x1, x2 = x_range\n","    y1, y2 = y_range\n","    img_array_patched = img_array.copy()\n","    img_array_patched[:,y1:y2,x1:x2,:]=0\n","    img_normalized = ((img_array_patched - img_array_patched.min()) / (img_array_patched.max() - img_array_patched.min()) * 255).astype(np.uint8).squeeze()\n","    plt.imshow(img_normalized, vmin=0, vmax=255),plt.show();\n","\n","    preds = model.predict(img_array_patched, verbose=0)\n","    top_5_preds = decode_predictions(preds, top=5)[0]\n","    data = []\n","    for pred in top_5_preds:\n","        data.append([pred[1], pred[2]])\n","    print(tabulate(data, headers=[\"Class\", \"P(y|x)\"], tablefmt=\"pipe\"))\n"],"metadata":{"id":"agrHe_tpUqYg","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5BHO3CRunwBx"},"source":["#Model selection & Weight loading\n","\n","Excute the cell below, choose a given architecture and press \"OK\" to load\n","\n"," (it might take some seconds to load , to change to another model, excecute the cell again):\n","\n","\n","The weights of the models trained on imagenet are automatically loaded:\n","\n","- Xception: https://arxiv.org/abs/1610.02357\n","- Resnet :  https://arxiv.org/abs/1512.03385\n","- VGG16:    https://arxiv.org/abs/1409.1556v6\n","\n","Check out the size of the models how many parameters are used?\n"]},{"cell_type":"code","source":["display(model_dropdown, ok_button)\n"],"metadata":{"id":"QM1KxxiGhYXF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## imagenet classes\n","The models we loaded are pretrained on imagenet with 1000 classes, with the dropdown menu below you can browse through the classes and find the according index e.g. 386 : \"African elephant\"\n"],"metadata":{"id":"JPKAM5L2HamI"}},{"cell_type":"code","source":["display(class_dropdown)"],"metadata":{"id":"GtnF6uUeqkS-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## image\n","Lets choose an african elephant image from the internet\n","Original link:\"https://upload.wikimedia.org/wikipedia/commons/3/37/African_Bush_Elephant.jpg\""],"metadata":{"id":"HWSYJ-nWHTHv"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"TZEdGF4JnwBy"},"outputs":[],"source":["img_path = keras.utils.get_file(\"african_elephant.jpg\",\"https://github.com/tensorchiefs/dl_course_2024/blob/main/notebooks/African_Bush_Elephant.jpg\")\n","display(Image(img_path, width=300))"]},{"cell_type":"markdown","source":["## forward pass\n","the original image gets preprocessed & passed to the model , display the top 5 predictions"],"metadata":{"id":"4v7t92STC2Tp"}},{"cell_type":"code","source":["img_array_orig = get_img_array(img_path, size=img_size)\n","img_array = preprocess_input(img_array_orig.copy())\n","preds = model.predict(img_array, verbose=0)\n","top_5_preds = decode_predictions(preds, top=5)[0]\n","data = []\n","for pred in top_5_preds:\n","    data.append([pred[1], pred[2]])\n","print(tabulate(data, headers=[\"Class\", \"P(y|x)\"], tablefmt=\"pipe\"))"],"metadata":{"id":"V_TgdK4qM-HZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The model predicts the class correctly. Similar classes which are also more likely than others :\n","- (101 tusker)\n","- (385 indian elephant)\n"],"metadata":{"id":"Ty1H_pozOGUm"}},{"cell_type":"markdown","source":["# xAI\n","\n","now that we have set everything up. lets inspect the regions of the image which lead the model to assign a high likelihood to belong to the class African Elephant.\n","\n"],"metadata":{"id":"R8IAVw6tPMaX"}},{"cell_type":"markdown","metadata":{"id":"Riy1I__yUku5"},"source":["# GradCam"]},{"cell_type":"markdown","source":["The gradcam algorithm, derives the likelihoodfunction for the selected class for each value in the activation maps in the layer we have selected (here last conv layer), weighs them by the average value per activation map, averages the sum of each of the maps and runs it through a relu function which sets all the negative values to 0. We end up with an aggregated map over all the activationmaps. This map is then rescaled to the original size of the image and superimposed on the original image for human inspection.\n"],"metadata":{"id":"cVKLDLNQSaCS"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5jfQt2fGUku5"},"outputs":[],"source":["# get last convolutional layer & size\n","last_conv_layer_name = [vis_layer for vis_layer in [i.name for i in model.layers] if \"conv\" in vis_layer][-1]\n","print(f'name of the last layer: {last_conv_layer_name}')\n","print(f'Activation map size and number of maps: {model.get_layer(last_conv_layer_name).output.shape}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GB8-Y-8YUku6"},"outputs":[],"source":["# Generate class activation heatmap\n","prediction_index=386 # african elephant\n","heatmap_gradcam = make_gradcam_heatmap(img_array, model, last_conv_layer_name,pred_index=prediction_index)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5U6lz2uUUku6"},"outputs":[],"source":["# generate superimposed image\n","superimposed_img_path=os.path.join(os.path.split(img_path)[0],\"superimposed_\"+os.path.split(img_path)[1])\n","print(superimposed_img_path)\n","generate_superimposed_image(img_array_orig, heatmap_gradcam, save_name = superimposed_img_path )"]},{"cell_type":"markdown","metadata":{"id":"cYrbRsVGnwB0"},"source":["# Occlusion by hand\n","\n","Another way of inspecting which regions of  the image contribute to the likelihood of an instance belonging to a class, is to occlude a part of the image.\n","\n","## Excercise\n","Move the sliders below to occlude a part of the input image and see how the predictions change.\n","\n","-  Can you find a spot which is marginally influencing the prediction?\n","-  or a spot where the likelihood is getting even higher for elephant??\n","-  what happens if you make a rectangle which is standing up?\n","\n","\n"," **Hint**:If you've chosen the vgg model dont worry if the colors looks a bit strange. The color channels of the image are bgr instead of rgb, since the creators of VGG trained it in this way"]},{"cell_type":"code","source":["x_slider = widgets.IntRangeSlider(value=[100, 180],min=0,max=img_array.shape[1]-1,step=1,description='X Range:',continuous_update=False)\n","y_slider = widgets.IntRangeSlider(value=[100, 180],min=0,max=img_array.shape[1]-1,step=1,description='Y Range:',continuous_update=False)\n","widgets.interact(patch_predict_image, x_range=x_slider, y_range=y_slider);"],"metadata":{"id":"B-R9IGVHgTKz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Answer occlusion"],"metadata":{"id":"mofd2peZqdGE"}},{"cell_type":"markdown","source":["\n","\n","- if a small patch in the topleft corner (amongst many others) is occluded there is almost no influence on the prediciton\n","\n","- Cover the upper 5th of the image or the lower 3rd and see that the likelihood for elephant increases. If all the other classes get more and more unlikely the more likely there is an elephant , iff the important regions for elephant are not occluded.\n","\n","- if you make a large enough standing up rectangle the image gets classified as a megalith , which is quite reasonable\n"],"metadata":{"id":"iUJgHV3hoe8s"}},{"cell_type":"markdown","source":["## Occlusion Algorithm\n","\n","now we can do it systematically, by defining a patch size and a stride to generate a set of occluded images."],"metadata":{"id":"d5I53hH_qTKN"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"90s_2g-w1n2j"},"outputs":[],"source":["if img_size[0]==224:\n","  occ_size = 26\n","  occ_stride = 18\n","if img_size[0]==299:\n","  occ_size = 35\n","  occ_stride = 22\n","\n","print(\"number of occlusions: \", int(np.prod(((np.array(img_array.shape[1:3]) - occ_size) / occ_stride) + 1)))\n","print((np.asarray(img_array[0,:,:,0].shape) - occ_size) % occ_stride)"]},{"cell_type":"markdown","source":["\n","\n","For each of the occluded images, the prediction for the chosen class is now stored as a value for the occluded patch, for overlapping patches an average is calculated.\n","As seen before, some patches even make the probability for the chosen class larger, therefore we just consider the patches which make the predicition worse than the prediction without occlusion.\n","\n","**execute the cell below**! Depending on how many occlusions are used, this calculation takes some time.\n","\n","While this is calculating try to think of what a human would look at to identify an african elephant, would you still be able to classify correctly if parts of the image are occluded?"],"metadata":{"id":"BpPiOMa4rDpB"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"WRuPYnP_Uku5"},"outputs":[],"source":["prediction_index=386\n","# generate occlusion class activation map\n","heatmap_occlusion =  rgb_image_occlusion(\n","    img_array[0], model_2d=model, correct_class = prediction_index,\n","    occlusion_size = occ_size, occlusion_stride = occ_stride\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0RkDaMb7Uku5"},"outputs":[],"source":["# generate superimposed image\n","occluded_img_path=os.path.join(os.path.split(img_path)[0],\"occlusion_\"+os.path.split(img_path)[1])\n","print(occluded_img_path)\n","generate_superimposed_image(img_array_orig, heatmap_occlusion, save_name =occluded_img_path )"]},{"cell_type":"markdown","source":[". The resulting heatmap displays the deviation from the prediction without occlusion. (The worse, the more red)"],"metadata":{"id":"FBJCdOQ56bxK"}},{"cell_type":"markdown","source":["# Excercise: Now it's your turn\n","\n","* Look up the activation heatmap for 'car mirror', does it make sense? You'll\n","* What happens if instead of a picture of a real elephant you use a picture of an elephant drawing, a plush or a model elephant. Does the activation heatmap still makes any sense? (e.g. https://dinosours.files.wordpress.com/2015/09/img_4952.jpg)\n","* Try to use the algorithm for a picture of your choice\n","* What happens if you change the visualized layer (last_con_layer_name)?\n","* What happens if two objects of the same class are in one image?\n","* Try to use the algorithm for another Convolutional Neural Network (e.g. VGG Net or your own)\n"],"metadata":{"id":"cv3poibRfPkw"}},{"cell_type":"markdown","source":["## HINTS\n","\n","If this notebook is opened in colab your can upload you image with the function:\n","\n","```\n","img_path='YOUR_FILENAME.jpg'\n","upload_image_with_filename(img_path)\n","```\n","\n","if you  are on a local instance just specify img_path\n","\n","```\n","img_path= \"/path/to/your/image/YOUR_FILENAME.jpg\"\n","```\n","\n","To get an image from the web just change the url:\n","\n","\n","```\n","img_path = keras.utils.get_file(\n","    \"YOUR_FILENAME.jpg\",\n","    \"https://storage.googleapis.com/petbacker/images/blog/2017/dog-and-cat-cover.jpg\",\n","    )\n","```\n","\n"],"metadata":{"id":"JiPKz6eCfUdA"}},{"cell_type":"markdown","source":["upload own images"],"metadata":{"id":"3W4RWoKG-r7N"}},{"cell_type":"code","source":["img_path='YOUR_FILENAME.jpg'\n","upload_image_colab(img_path)"],"metadata":{"id":"SfBm61IsfXko"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"https://github.com/keras-team/keras-io/blob/master/examples/vision/ipynb/grad_cam.ipynb","timestamp":1653472170736}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}