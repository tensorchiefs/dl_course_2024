{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"cells":[{"cell_type":"markdown","metadata":{"id":"whgi7gogh-1J"},"source":["\n","#Numerical Stability\n","\n","**Goal:**\n","In this experiment you will investigate the numerical stability of the product of probabilities and compare it with the (improved) stability of the sum of the logs of these probailities. With this you get a feeling why optimizing the joint log-likelihood is computationally more stable than optimizing the joint likelihood.\n","\n","**Usage:** For additional Information read chapter 4 of the [Probabilistic Deeplearning book](https://www.manning.com/books/probabilistic-deep-learning?a_aid=probabilistic_deep_learning&a_bid=78e55885).\n","\n","**Content:**\n","* show that calculating the product of many probabilities (which are <= 1) leads to numerical instabilities which are not observed when calculating the sum of the log of these probabilities.\n"]},{"cell_type":"code","metadata":{"id":"qmWNBQnTGQMn"},"source":["try: #If running in colab\n","    import google.colab\n","    IN_COLAB = True\n","    %tensorflow_version 2.x\n","except:\n","    IN_COLAB = False\n","\n","import tensorflow as tf\n","if (not tf.__version__.startswith('2')): #Checking if tf 2.0 is installed\n","    print('Please install tensorflow 2.0 to run this notebook')\n","print('Tensorflow version: ',tf.__version__, ' running in colab?: ', IN_COLAB)\n","\n","#load required libraries:\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","plt.style.use('default')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sbV7rIX4lcxz"},"source":["# Numerical Stability\n","\n","\n","### The product of probabilities.\n","\n","To calculate the joint likelihood you have to determine the product of many probabilities. As you can see in the following, mulitiplying many values between zero and one, leads to very small values which is set to zero in python, if the number gets too small.   \n","To demonstrate this, we sample 100 values from an uniform distribution with min = 0 and max = 1, then we take the product of those values and do the same for  1000 values.\n"]},{"cell_type":"code","metadata":{"id":"SRnBJcTLh67y"},"source":["vals100 = np.random.uniform(0,1,100)\n","vals1000 = np.random.uniform(0,1,1000)\n","x100 = np.product(vals100)\n","x1000 = np.product(vals1000)\n","print(f'product of 100  samples: {x100}',f'\\nproduct of 1000 samples: {x1000}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MWRhK045vSf1"},"source":["When multiplying 100 values you get a very very small number but for 1000 values you get 0.0, this is due to the limited precision of the float numbers in a computer. But this is a real problem, because it looks like that joint likelihood is zero, but its not (its just very small due to the large amount of data).\n"]},{"cell_type":"markdown","metadata":{"id":"eFQstp-QmMmw"},"source":["### Taking the log does not change the position of the maximum\n","\n","In the next cell we show that the x value which gives the position of the maximum of the function f(x), gives also the position of the maximum of log(f(x)). For demonstration, we use the absolute values of the product of two sine waves as our function f(x) and take the log of it."]},{"cell_type":"code","metadata":{"id":"p91gzNLKlv7A"},"source":["vals = 1 + np.abs(np.sin(np.linspace(0, 3*np.pi, 1000)) * np.sin(np.linspace(0, np.pi, 1000)))\n","plt.plot(range(0, 1000),vals,'b-')\n","plt.plot(range(0, 1000),np.log(vals),'g--')\n","plt.xlabel('x')\n","plt.legend((\"f(x)\",\"log(f(X))\"),fontsize=10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qJ4BoYmTy1S8"},"source":["Here it is clearly visible: the maximum for both functions is at the same position."]},{"cell_type":"markdown","metadata":{"id":"tROj3cUXn_Pn"},"source":["### Takeing the logs and summing up.\n","\n","If we take the log of a product it leads to $\\log(A \\cdot B) = \\log(A) + \\log(B)$  meaning that we can work with a sum of the logs (see book).  \n","\n","Now you apply a log to the product of probabilities which gives you a sum of logs of these probabilities. Remember we have values from a uniform distribution with min = 0 and max = 1 (probabilities), which lead to numerical problems when calculating the product of theses probabilities.\n","As you can see now, on the log scale you don't have the problem of the numerical precision anymore."]},{"cell_type":"markdown","metadata":{"id":"_v82NZ_zG5Zy"},"source":["#### Listing 4.3 Fixing the numerical instabilities by taking the log                                                                                                                                                                                                                        \n"]},{"cell_type":"code","metadata":{"id":"GLe4jtAolzId"},"source":["import numpy as np\n","log_x100 = np.sum(np.log(vals100))\n","log_x1000 = np.sum(np.log(vals1000))\n","log_x100, log_x1000\n","# The product becomes the sum of the logs\n","print(f'log of product, 100  samples: {log_x100}',f'\\nlog of product, 1000 samples: {log_x1000}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q1KqCtQkyQLR"},"source":["This result is quite important for implementing the maximum likelihood estimation procedure. In the maximum likelihood approach you want to determine the parameter value that yields the highest joint likelihood over all observed data. The very same parameter value will also maximize the joint log-likelihood. However, if you have a lot of data, the likelihood cannot be precisely determined, but the log-likelihood can. This is the reason why in DL you work with the negative log-likelihood as loss function instead of the negative likelihood."]}]}