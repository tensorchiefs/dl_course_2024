{"cells":[{"cell_type":"markdown","metadata":{"id":"4K8Ug6ICkRtQ"},"source":["# A simple CNN for the edge lover task\n","\n","In this notebook you train a very simple CNN with only 1 kernel to distinguish between images containing vertical and images containing horizontal stripes. To check what pattern is recognized by the learned kernel you will visualize the weights of the kernel as an image. You will see that the CNN learns a useful kernel (either a vertical or horiziontal bar). You can experiment with the code to check the influence of the kernel size, the activation function and the pooling method on the result.  \n","\n","\n","**Dataset:** You work with an artficially generated dataset of greyscale images (50x50 pixel) with 10 vertical or horizontal bars. We want to classify them into whether an art lover, who only loves vertical strips, will like the image (y = 0) or not like the image (y = 1).  \n","\n","The idea of the notebook is that you try to understand the provided code by running it, checking the output and playing with it by slightly changing the code and rerunning it.  \n","\n","**Content:**\n","* definig and generating the dataset X_train and X_val\n","* visualize samples of the generated images\n","* use keras to train a CNN with only one kernel (5x5 pixel)\n","* visualize the weights of the learned kernel and interpret if it is useful\n","* repeat the last two steps to check if the learned kernel is always the same\n","\n"]},{"cell_type":"markdown","metadata":{"id":"eiB8bJNYn8oP"},"source":["### Imports\n","\n","In the next cell, we load all the required libraries."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2PDLAWRQ7iUB"},"outputs":[],"source":["# load required libraries:\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","plt.style.use('default')\n","\n","import tensorflow.keras\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Convolution2D, MaxPooling2D, Flatten , Activation\n","from tensorflow.keras.utils import to_categorical"]},{"cell_type":"markdown","metadata":{"id":"Oq0FNqcBpj23"},"source":["### Defining functions to generate images\n","\n","Here we define the function to genere images with vertical and horizontal bars, the arguments of the functions are the size of the image and the number of bars you want to have. The bars are at random positions in the image with a random length. The image is black and white, meaning we have only two values for the pixels, 0 for black and 255 for white."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nqVBlR8yAO9c"},"outputs":[],"source":["#define function to generate image with shape (size, size, 1) with stripes\n","def generate_image_with_bars(size, bar_nr, vertical = True):\n","    img = np.zeros((size,size,1), dtype=\"uint8\")\n","    for i in range(0,bar_nr):\n","        x,y = np.random.randint(0,size,2)\n","        l  = int(np.random.randint(y,size,1)[0])\n","        if (vertical):\n","            img[y:l,x,0]=255\n","        else:\n","            img[x,y:l,0]=255\n","    return img"]},{"cell_type":"markdown","metadata":{"id":"bUmdGzQLdqzB"},"source":["Let's have a look at the generated images. We choose a size of 50x50 pixels and set the number of bars in the image to 10."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EccLz0FlXGuU"},"outputs":[],"source":["# have a look on two generated images\n","plt.figure(figsize=(8,8))\n","plt.subplot(1,2,1)\n","img=generate_image_with_bars(50,10, vertical=True)\n","plt.imshow(img[:,:,0],cmap='gray')\n","plt.subplot(1,2,2)\n","img=generate_image_with_bars(50,10, vertical=False)\n","plt.imshow(img[:,:,0],cmap='gray')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Y8gSwmyaevTk"},"source":["### Make a train and validation dataset of images with vertical and horizontal images\n","Now, let's make a train dataset *X_train* with 1000 images (500 images with vertical and 500 images with horizontal bars). We normalize the images values to be between 0 and 1 by dividing all values with 255. We create a secont dataste *X_val* with exactly the same properties to validate the training of the CNN."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"63omuptEILKu"},"outputs":[],"source":["pixel=50  # define height and width of images\n","num_images_train = 1000 #Number of training examples (divisible by 2)\n","num_images_val = 1000 #Number of training examples (divisible by 2)\n","\n","# generate training data with vertical edges\n","X_train =np.zeros((num_images_train,pixel,pixel,1))\n","for i in range(0, num_images_train//2):\n","    X_train[i]=generate_image_with_bars(pixel,10)\n","# ... with horizontal\n","for i in range(num_images_train//2, num_images_train):\n","    X_train[i]=generate_image_with_bars(pixel,10, vertical=False)\n","\n","# generate validation data with vertical edges\n","X_val =np.zeros((num_images_train,pixel,pixel,1))\n","for i in range(0, num_images_train//2):\n","    X_val[i]=generate_image_with_bars(pixel,10)\n","# ... with horizontal\n","for i in range(num_images_train//2, num_images_train):\n","    X_val[i]=generate_image_with_bars(pixel,10, vertical=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oBQjP6pZxMfa"},"outputs":[],"source":["# normalize the data to be between 0 and 1\n","X_train=X_train/255\n","X_val=X_val/255\n","\n","print(X_train.shape)\n","print(X_val.shape)"]},{"cell_type":"markdown","metadata":{"id":"ajNnUoYyi7IQ"},"source":["Here we make the labels for the art lover, 0 means he likes the image (vertical bars) and 1 means that he doesn't like it (horizontal stripes). We one hot encode the labels because we want to use two outputs in our network."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"41-L5hM8S_ZP"},"outputs":[],"source":["# create class labels\n","y = np.array([[0],[1]])\n","Y_train = np.repeat(y, num_images_train //2)\n","Y_val = np.repeat(y, num_images_train //2)\n","\n","# one-hot-encoding\n","Y_train = to_categorical(Y_train,2)\n","Y_val = to_categorical(Y_val,2)"]},{"cell_type":"markdown","metadata":{"id":"uZpr0h-VvatF"},"source":["## Defining the CNN\n","\n","Here we define the CNN:\n","\n","- we use only one kernel with a size of 5x5 pixels  \n","- then we apply a linar activation function  \n","- the maxpooling layer takes the maximum of the whole activation map to predict the probability (output layer with softmax) if the art lover will like the image\n","\n","As loss we use the categorical_crossentropy and we train the model with a batchsize of 64 images per update.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Dfg1h2rUifd"},"outputs":[],"source":["model = Sequential()\n","\n","model.add(Convolution2D(1,(5,5),padding='same',input_shape=(pixel,pixel,1)))\n","model.add(Activation('linear'))\n","\n","# take the max over all values in the activation map\n","model.add(MaxPooling2D(pool_size=(pixel,pixel)))\n","model.add(Flatten())\n","model.add(Dense(2))\n","model.add(Activation('softmax'))\n","\n","# compile model and initialize weights\n","model.compile(loss='categorical_crossentropy',\n","              optimizer='adam',\n","              metrics=['accuracy'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r6eqV0TRU0_n"},"outputs":[],"source":["# let's summarize the CNN architectures along with the number of model weights\n","model.summary()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sc-BYd8kVCx0","scrolled":false},"outputs":[],"source":["# train the model\n","history=model.fit(X_train, Y_train,\n","                  validation_data=(X_val,Y_val),\n","                  batch_size=64,\n","                  epochs=150,\n","                  verbose=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fK_AAAoiQtlc"},"outputs":[],"source":["# plot the development of the accuracy and loss during training\n","plt.figure(figsize=(12,4))\n","plt.subplot(1,2,(1))\n","plt.plot(history.history['accuracy'],linestyle='-.')\n","plt.plot(history.history['val_accuracy'])\n","plt.title('model accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'valid'], loc='lower right')\n","plt.subplot(1,2,(2))\n","plt.plot(history.history['loss'],linestyle='-.')\n","plt.plot(history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'valid'], loc='upper right');"]},{"cell_type":"markdown","metadata":{"id":"uOwR3Esbw8eN"},"source":["### Visualize the learned kernel and experiment with the code\n","\n","You see that the CNN performs very good at this task (100% accuracy). We can check which pattern is recognized by the **learned kernel** and see if you think that this is helpful to distinguish between images with horizontal and vertical edges.\n","\n","Below you can see the original image, the image after the convolution operation with the learned kernel and the maximum value from the maxpooling operation. Note that the maxpooling has the same size as the convolved image so there is just one value as output.\n","\n","The probability (or more exactly likelihood) assigned by the model for each sample to belong to class \"horizontal\" or \"vertical\" is displayed P(y=\"horizontal\"|x),P(y=\"vertical\"|x) . The -log(P(y=\"horizontal\"|x)) or -log(P(y=\"vertical\"|x))  (the NLL : NegativeLogLikelihoods) which are used in the loss function \n","are displayed aswell. \n","\n","\n","\n","Move the sliders to inspect different pictures from the validation set and their predictions\n","\n","\n","#### Extra\n","\n","try to lower (or increase) the training epochs and see how the -log(P(y|x)) change."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pl1yuAddVRnE"},"outputs":[],"source":["## Do not worry about this cell, just move the sliders.\n","import scipy.signal\n","from skimage.measure import block_reduce  # For max pooling\n","import ipywidgets as widgets\n","\n","# Kernel from model\n","plt.figure(figsize=(10, 3))\n","plt.subplot(1, 2, 1)\n","plt.imshow(np.random.rand(25).reshape(5, 5),\"gray\") ,plt.title('Randomly initalized weights')\n","plt.subplot(1, 2, 2)\n","conv_filter=np.squeeze(model.get_weights()[0], axis=2)\n","plt.imshow(conv_filter[:,:,0],\"gray\"),plt.title('Learned Kernel (weights) , by model'),plt.show();\n","print(\"\\n---------Move the sliders to inspect different vertical and horizontal images from the valset and their predictions:------------------\\n\")\n","\n","def scale_convolution_map(conv_map, min_val=-3, max_val=3):\n","    clipped_conv_map = np.clip(conv_map, min_val, max_val)\n","    scaled_conv_map = (clipped_conv_map - min_val) / (max_val - min_val)\n","    return scaled_conv_map\n","\n","def plot_conv(img):\n","  convolved_image = scipy.signal.convolve2d(img.squeeze(), conv_filter.squeeze(), mode='same')\n","  scaled_conv_image = scale_convolution_map(convolved_image + model.get_weights()[1])\n","  max_pooled_image = block_reduce(convolved_image + model.get_weights()[1], block_size=(50, 50), func=np.max)\n","  scaled_max_pooled_image = scale_convolution_map(max_pooled_image)\n","  \n","  plt.figure(figsize=(20, 4))  # Adjust the figure size as needed\n","  plt.subplot(1, 6, 1)\n","  plt.imshow(img, \"gray\", vmin=0, vmax=1),plt.title('Original Image')\n","  plt.subplot(1, 6, 2)\n","  plt.imshow(scaled_conv_image, \"gray\", vmin=0, vmax=1),plt.title('Convolved Image')\n","  plt.subplot(1, 6, 3),plt.imshow(scaled_max_pooled_image, \"gray\", vmin=0, vmax=1)\n","  plt.title(f'Max Pooled = {max_pooled_image[0][0]:.2f}'),plt.xticks([]), plt.yticks([])\n","  plt.subplot(1, 6, 4),plt.axis('off')\n","  pred = model.predict(img.reshape(1, 50, 50, 1), verbose=0)\n","  text_info = f'''\n","  P(y=vertical|x): {pred[0][0]:.4f}\n","  P(y=horizontal|x): {pred[0][1]:.4f}\n","                  \n","                  \n","   -log(P(y=vertical|x)): {-np.log(pred[0][0]):.4f}\n","   -log(P(y=horizontal|x)): {-np.log(pred[0][1]):.4f}\n","    '''\n","  plt.text(0, 0.5, text_info, ha='left', va='center')\n","  plt.subplot(1, 6, 5)\n","  x_values = np.linspace(0.001, 1.1, 500)\n","  plt.plot(x_values, -np.log(x_values), label='-log(P(y|x))')\n","  plt.ylim(-0.5, 6),plt.xlim(-0.1, 1.1),plt.xlabel('P(y|x)')\n","  plt.plot(pred[0][0], -np.log(pred[0][0]), 'bo', label='-log(P(y=vertical|x))')\n","  plt.plot(pred[0][1], -np.log(pred[0][1]), 'ro', label='-log(P(y=horizontal|x))')\n","  plt.legend(),plt.grid(True), plt.tight_layout(),plt.show();\n","\n","def inspect_preds(horizontal,vertical):\n","  plot_conv(X_val[horizontal,:,:,0])\n","  plot_conv(X_val[vertical,:,:,0])\n","\n","horizontal_slider = widgets.IntSlider(min=0, max=num_images_val//2-1, step=1, value=0, description='vertical ')\n","vertical_slider = widgets.IntSlider(min=num_images_val//2, max=num_images_val-1, step=1, value=0, description='horizontal')\n","widgets.interact(inspect_preds, horizontal=horizontal_slider, vertical=vertical_slider);"]},{"cell_type":"markdown","metadata":{"id":"U4gnnlAPp_Q2"},"source":["### Repeat the training and experiment with the kernelsize and activation function.\n","\n","**Exercise**:\n","- Repeat the compiling and training, beginning from the cell:\n","\n","```\n","model = Sequential()\n","       \n","       ...\n","       \n","model.compile(loss='categorical_crossentropy',\n","              optimizer='adam',\n","              metrics=['accuracy'])\n","```\n","\n","for several times and check if the CNN always learns the same kernel.  \n","\n","- You can experiment with the code and check what happens if you use another kernel size, activation function (relu instead of linear ) or pooling method AveragePooling instead of MaxPooling.  Try to make a prediction on the performance before doing the experiment.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fRlCUwpVoy69"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.11"}},"nbformat":4,"nbformat_minor":0}
