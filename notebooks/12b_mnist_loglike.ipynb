{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tbTYFRhJoaBu"
   },
   "source": [
    "# Calculation of the cross entropy loss (NLL) for a classification tasks\n",
    "\n",
    "\n",
    "**Goal:** In this notebook you will use Keras to set up a CNN for classification of MNIST images and calculate the cross entropy before the CNN was trained. You will use basic numpy functions to calculate the loss that is expected from random guessing and see that an untrained CNN is not better than guessing.\n",
    "\n",
    "**Usage:** The idea of the notebook is that you try to understand the provided code by running it, checking the output and playing with it by slightly changing the code and rerunning it. \n",
    "\n",
    "**Dataset:** You work with the MNIST dataset. You have 60'000 28x28 pixel greyscale images of digits (0-9).\n",
    "\n",
    "**Content:**\n",
    "* load the original MNIST data \n",
    "* define a CNN in Keras\n",
    "* evaluation of the cross entropy loss function of the untrained CNN for all classes\n",
    "* implement the loss function yourself using the predicted probabilities and numpy\n",
    "\n",
    "\n",
    "| [open in colab](https://colab.research.google.com/github/tensorchiefs/dl_book/blob/master/chapter_04/nb_ch04_02.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqKCMDTIL-A0"
   },
   "source": [
    "#### Install correct TF version (colab only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SaKmqwkUL-4K",
    "outputId": "c8a23f6a-c927-48c3-d2d3-d62266bee1b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-2.0.2 tensorflow-2.0.0 tensorflow-estimator-2.0.1\n"
     ]
    }
   ],
   "source": [
    "# Execute this cell to be sure to have a compatible TF (2.0) version. \n",
    "# If you are bold you can skip this cell. \n",
    "try: #If running in colab \n",
    "  import google.colab\n",
    "  !pip install tensorflow==2.0.0\n",
    "except:\n",
    "  print('Not running in colab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PEIS4WvpsT5t"
   },
   "source": [
    "#### Imports\n",
    "\n",
    "First you load all the required libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-FOsctrBLfGa"
   },
   "outputs": [],
   "source": [
    "try: #If running in colab \n",
    "    import google.colab\n",
    "    IN_COLAB = True \n",
    "    %tensorflow_version 2.x\n",
    "except:\n",
    "    IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-GphWmaNLfGg",
    "outputId": "7db22392-be3b-4391-b30d-a127c8ec99bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version:  2.0.0  running in colab?:  True\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "if (not tf.__version__.startswith('2')): #Checking if tf 2.0 is installed\n",
    "    print('Please install tensorflow 2.0 to run this notebook')\n",
    "print('Tensorflow version: ',tf.__version__, ' running in colab?: ', IN_COLAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Y6S_hQX5oaBw",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load required libraries:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('default')\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Convolution2D, MaxPooling2D, Flatten , Activation\n",
    "from tensorflow.keras.utils import to_categorical \n",
    "from tensorflow.keras import optimizers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4h_3TS0CtJJb"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Loading and preparing the MNIST data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4sZ8lqFfoaB2",
    "outputId": "c0626bfb-5572-4768-9b9f-4d1ca2027184"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((60000, 10), (60000, 28, 28, 1))"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train=x_train / 255 #divide by 255 so that they are in range 0 to 1\n",
    "X_train=np.reshape(X_train, (X_train.shape[0],28,28,1))\n",
    "Y_train=tensorflow.keras.utils.to_categorical(y_train,10) # one-hot encoding\n",
    "\n",
    "\n",
    "\n",
    "Y_train.shape, X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaRFUEP8HJkq"
   },
   "source": [
    "## CNN model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "JSfYQ4f1KYVp"
   },
   "outputs": [],
   "source": [
    "# here you define hyperparameter of the CNN\n",
    "batch_size = 128\n",
    "nb_classes = 10 \n",
    "img_rows, img_cols = 28, 28\n",
    "kernel_size = (3, 3)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "pool_size = (2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "f0q16a2uIBkg"
   },
   "outputs": [],
   "source": [
    "\n",
    "# define CNN with 2 convolution blocks and 2 fully connected layers\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(8,kernel_size,padding='same',input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(8, kernel_size,padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "model.add(Convolution2D(16, kernel_size,padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(16,kernel_size,padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(40))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# compile model and intitialize weights\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dWx9gqJ6IUpZ",
    "outputId": "1dfda0e8-4579-4bb9-c378-db7ce33b1672"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 8)         80        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 28, 28, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 8)         584       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 28, 28, 8)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 16)        1168      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 14, 16)        2320      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 40)                31400     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                410       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 35,962\n",
      "Trainable params: 35,962\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# summarize model along with number of model weights\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3-mY30BBI4LJ"
   },
   "source": [
    "Here you predict the probabilities for all images in the training data set. You did not train the network yet, therefore the probabilities will be around 10% for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "430DSTDIHJlP"
   },
   "outputs": [],
   "source": [
    "# Calculate the probailities for the training data\n",
    "Pred_prob = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TTyxe7xMJUKC",
    "outputId": "88037c52-f179-4f62-f960-9aaf8681b106"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.09725589, 0.10157428, 0.0950794 , 0.09868431, 0.10703634,\n",
       "        0.10252   , 0.09724157, 0.09794403, 0.10431638, 0.09834775],\n",
       "       [0.0944576 , 0.0993908 , 0.09671719, 0.09919823, 0.10408939,\n",
       "        0.10388145, 0.0982025 , 0.10004048, 0.1063941 , 0.09762837],\n",
       "       [0.09994099, 0.09989374, 0.09601263, 0.09482878, 0.10330015,\n",
       "        0.10193894, 0.0983932 , 0.10026404, 0.10553848, 0.09988905],\n",
       "       [0.09862671, 0.10060232, 0.10047358, 0.09748498, 0.10155009,\n",
       "        0.09927452, 0.10024472, 0.09830353, 0.10395879, 0.09948073],\n",
       "       [0.09877217, 0.09976703, 0.09861047, 0.09711061, 0.09931648,\n",
       "        0.10188781, 0.09807245, 0.1001584 , 0.10730772, 0.09899686]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pred_prob[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XcqY_UbNYyP2",
    "outputId": "0d3f785a-f25e-4d2b-89c1-f81e9138e1dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 10), (60000, 10))"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pred_prob.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9W46_Euob-ux"
   },
   "source": [
    "### Exercise : Calculate the loss function using numpy\n",
    "<img src=\"https://raw.githubusercontent.com/tensorchiefs/dl_book/master/imgs/paper-pen.png\" width=\"60\" align=\"left\" />  \n",
    "\n",
    "*Exercise : Use numpy to calculate the value of the negative log-likelihood loss (=cross entropy) that you expect for the untrained CNN, which you have constructed above to discriminate between the 10 classes. Determine the cross entropy that results from the predicted probabilities (Pred_prob). To determine the cross entropy of the prediction, you can loop over each example and use its true label (Y_train) and the predicted probability for the true class. Do you get the cross entropy value that you have expected?*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n-nSWXYadTft"
   },
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hv1IEA74dPF6"
   },
   "source": [
    "Scroll down to see the solution.\n",
    "\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KM1EOk9WLkeh"
   },
   "source": [
    "In the next cell you calculate the cross entropy loss of each single image, then you sum up all individual losses and divide the sum with the nr of training examples. You take the negative of this result to get the NLL, also known as categorical cross entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v9GkdLKcY5OU",
    "outputId": "719c2f27-fc60-4c9e-a52b-b50bd19158ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3052261403361958"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss=np.zeros(len(X_train))\n",
    "Y=np.argmax(Y_train,axis=1)\n",
    "for i in range(0,len(X_train)):\n",
    "  loss[i]=np.log(Pred_prob[i][Y[i]])\n",
    "-np.sum(loss)/len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CxOvZwJiMZDg"
   },
   "source": [
    "If you have no idea about the training dataset, your guess for every image would be 1/nr_of_classes, in the case with 10 classes, you would predicit every image with a probability around 0.1. The corresponding NLL is calculated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oWml2J8MKqwQ",
    "outputId": "6ccd9abe-5fae-4625-8763-313a232786b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3025850929940455"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nr_of_classes=10\n",
    "-np.log(1/nr_of_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJElA61ZMeZM"
   },
   "source": [
    "You get more or less the same result as as you got with the model.evaluate function for the untrained CNN.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n60Ql16SLZac",
    "outputId": "e0f6a5c4-19d4-43b3-d233-d1d5af805e96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/1 - 19s - loss: 2.3059 - accuracy: 0.0830\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.3052261344909666, 0.083]"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_train, Y_train,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UMZLeO25MvKB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "12b_mnist_loglike.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
