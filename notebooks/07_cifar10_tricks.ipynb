{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tbTYFRhJoaBu"
   },
   "source": [
    "# Cifar10 classification tricks\n",
    "\n",
    "In this notebook you will download the cifar10 dataset which contains quite small images (32x32x3) of 10 classes. The data is from the Canadian Institute For Advanced Research. You will plot examples of the images with the class label. Note that because the images are so small it is not always very easy to recognise which of the ten classes is on the image, even as a human. After loading the dataset you will train multiple models and compare the performances of the models on the testset.\n",
    "\n",
    "**Dataset:**  You work with the Cifar10 dataset. You have 60'000 32x32 pixel color images of 10 classes (\"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\"dog\",\"frog\",\"horse\",\"ship\",\"truck\")\n",
    "\n",
    "**Content:**\n",
    "* load the original cifar10 data create a train val and test dataset\n",
    "* visualize samples of cifar10 dataset\n",
    "\n",
    "* train a random forest on the pixelvalues\n",
    "* train a cnn from scratch without normalization\n",
    "* train a cnn from scratch with normalization\n",
    "* train a cnn from scratch with dropout\n",
    "* train a cnn from scratch with batchnorm\n",
    "* train a cnn from scratch with data augmentation\n",
    "\n",
    "* compare the performances of the models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PEIS4WvpsT5t"
   },
   "source": [
    "#### Imports\n",
    "\n",
    "In the next two cells, we load all the required libraries and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-0u353ydb9w2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load required libraries:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "plt.style.use('default')\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Convolution2D, MaxPooling2D, Flatten , Activation, Dropout, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical \n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y7kfDCYsxRVJ"
   },
   "source": [
    "\n",
    "### Load and plot the data\n",
    "\n",
    "In the next cell you will load the Cifar10 dataset, 50'000 images are in the training set and 10'000 are in the test dataset. You will use 10'000 for the train and validation dataset.\n",
    "You will plot one random example of each label and will see\n",
    "that the images are really small and finally you can convert the lables into the one hot encoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WRLfhCzVviwL"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "4zwRlaILb9xF",
    "outputId": "459c5eb9-1a92-411a-c366-4bcffba84d19"
   },
   "outputs": [],
   "source": [
    "# separate train val and test dataset\n",
    "X_train=x_train[0:10000] \n",
    "Y_train=to_categorical(y_train[0:10000],10) # one-hot encoding\n",
    "\n",
    "X_val=x_train[20000:30000] \n",
    "Y_val=to_categorical(y_train[20000:30000],10)\n",
    "\n",
    "X_test=x_test \n",
    "Y_test=to_categorical(y_test,10)\n",
    "\n",
    "del x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "colab_type": "code",
    "id": "o6jukEcMb9xL",
    "outputId": "11cfb9ad-913e-452e-941f-3fb302e336d3"
   },
   "outputs": [],
   "source": [
    "labels=np.array([\"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\"dog\",\"frog\",\"horse\",\"ship\",\"truck\"])\n",
    "#sample image of each label\n",
    "plt.figure(figsize=(15,15))\n",
    "for i in range(0,len(np.unique(np.argmax(Y_train,axis=1)))):\n",
    "    rmd=np.random.choice(np.where(np.argmax(Y_train,axis=1)==i)[0],1)\n",
    "    plt.subplot(1,10,i+1)\n",
    "    img=X_train[rmd]\n",
    "    plt.imshow(img[0,:,:,:])\n",
    "    plt.title(labels[i]+\" \"+str(np.argmax(Y_train,axis=1)[rmd][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OLX7w_4Zb9xP",
    "outputId": "c1f1a44e-1c06-4252-f84a-77c2aa013d5e"
   },
   "outputs": [],
   "source": [
    "# check the shape of the data\n",
    "X_train.shape, Y_train.shape, X_val.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z5rxBKcRdPKR"
   },
   "source": [
    "### RF on pixelvalues\n",
    "In this section you will train a random forest on the raw pixelvalues of the images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "gQnu_hIpcUp1",
    "outputId": "74fe6ba1-c008-4ee0-c39a-35b0d146df10"
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=40,random_state=22)\n",
    "clf.fit(X_train.reshape(len(X_train),32*32*3), np.argmax(Y_train,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "kY9AkXRrcUp-",
    "outputId": "0a709274-7158-4bdd-8855-14722b8494da"
   },
   "outputs": [],
   "source": [
    "pred=clf.predict(X_test.reshape(len(X_test),32*32*3))\n",
    "acc=np.average(pred==np.argmax(Y_test,axis=1))\n",
    "res1 = pd.DataFrame(\n",
    "          {'Acc' : acc}, index=['rf on pixelvalues'])\n",
    "res1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mi-o6-OagtZf"
   },
   "source": [
    "### CNN from scratch wihout normalization\n",
    "In this section you train a cnn from scratch to learn to classify the images into the right label. Normalization is not applied to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "colab_type": "code",
    "id": "PlXRWiwi8FHq",
    "outputId": "4c20b21d-571b-45a5-e7d0-336e9e66c7ed"
   },
   "outputs": [],
   "source": [
    "model  =  Sequential()\n",
    "\n",
    "model.add(Convolution2D(16,(3,3),activation=\"relu\",padding=\"same\",input_shape=(32,32,3)))\n",
    "model.add(Convolution2D(16,(3,3),activation=\"relu\",padding=\"same\"))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "\n",
    "model.add(Convolution2D(32,(3,3),activation=\"relu\",padding=\"same\"))\n",
    "model.add(Convolution2D(32,(3,3),activation=\"relu\",padding=\"same\"))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(300))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(100))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "colab_type": "code",
    "id": "-7B65EQTg6vM",
    "outputId": "8bc3a816-1bee-4820-9e8a-b5a758d6422a"
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train, \n",
    "                    batch_size=64,\n",
    "                    epochs=10, validation_data=(X_val, Y_val),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "colab_type": "code",
    "id": "aud3gKbNhT3b",
    "outputId": "6bb3bb6e-09a0-4e82-8988-00de9e316041"
   },
   "outputs": [],
   "source": [
    "# plot the development of the accuracy and loss during training\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,(1))\n",
    "plt.plot(history.history['accuracy'],linestyle='-.')\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='lower right')\n",
    "plt.subplot(1,2,(2))\n",
    "plt.plot(history.history['loss'],linestyle='-.')\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "colab_type": "code",
    "id": "RYco3ZFohT3r",
    "outputId": "507b573c-b4fe-4913-b2e4-afa9306f6c98"
   },
   "outputs": [],
   "source": [
    "acc=np.average(np.argmax(model.predict(X_test),axis=1)==np.argmax(Y_test,axis=1))\n",
    "res2 = pd.DataFrame(\n",
    "          {'Acc' : acc}, index=['cnn from scratch without normalization']\n",
    ")\n",
    "pd.concat([res1,res2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN from scratch with normalization\n",
    "In this section you train a cnn from scratch to learn to classify the images into the right label. Normalization is applied to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization\n",
    "X_train_norm = X_train/255\n",
    "X_val_norm = X_val/255\n",
    "X_test_norm = X_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model  =  Sequential()\n",
    "\n",
    "model.add(Convolution2D(16,(3,3),activation=\"relu\",padding=\"same\",input_shape=(32,32,3)))\n",
    "model.add(Convolution2D(16,(3,3),activation=\"relu\",padding=\"same\"))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "\n",
    "model.add(Convolution2D(32,(3,3),activation=\"relu\",padding=\"same\"))\n",
    "model.add(Convolution2D(32,(3,3),activation=\"relu\",padding=\"same\"))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(300))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(100))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_norm, Y_train, \n",
    "                    batch_size=64,\n",
    "                    epochs=10, validation_data=(X_val_norm, Y_val),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the development of the accuracy and loss during training\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,(1))\n",
    "plt.plot(history.history['accuracy'],linestyle='-.')\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='lower right')\n",
    "plt.subplot(1,2,(2))\n",
    "plt.plot(history.history['loss'],linestyle='-.')\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc=np.average(np.argmax(model.predict(X_test_norm),axis=1)==np.argmax(Y_test,axis=1))\n",
    "res3 = pd.DataFrame(\n",
    "          {'Acc' : acc}, index=['cnn from scratch with normalization']\n",
    ")\n",
    "pd.concat([res1,res2,res3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wIZH1xpehpOH"
   },
   "source": [
    "### CNN from scratch with Dropout\n",
    "In this section you train a cnn from scratch to learn to classify the images into the right label. This time you will use dropout layers in the classification part. Normalization is not used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 780
    },
    "colab_type": "code",
    "id": "QYD-TYOzhpOM",
    "outputId": "2f8562e3-5284-470f-c92e-620b50f6a569"
   },
   "outputs": [],
   "source": [
    "model  =  Sequential()\n",
    "\n",
    "model.add(Convolution2D(16,(3,3),activation=\"relu\",padding=\"same\",input_shape=(32,32,3)))\n",
    "model.add(Convolution2D(16,(3,3),activation=\"relu\",padding=\"same\"))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "\n",
    "model.add(Convolution2D(32,(3,3),activation=\"relu\",padding=\"same\"))\n",
    "model.add(Convolution2D(32,(3,3),activation=\"relu\",padding=\"same\"))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(500))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(300))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(100))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 728
    },
    "colab_type": "code",
    "id": "cfv_8EYChpOR",
    "outputId": "ec4a8d9f-1526-4365-af15-a9dd870a6bc2"
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train, \n",
    "                    batch_size=64,\n",
    "                    epochs=20, validation_data=(X_val, Y_val),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "colab_type": "code",
    "id": "4utU_Wd7hpOY",
    "outputId": "afb1c330-7507-4acb-d09f-ed3310d880e7"
   },
   "outputs": [],
   "source": [
    "# plot the development of the accuracy and loss during training\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,(1))\n",
    "plt.plot(history.history['accuracy'],linestyle='-.')\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='lower right')\n",
    "plt.subplot(1,2,(2))\n",
    "plt.plot(history.history['loss'],linestyle='-.')\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "7qAipwYkhpOf",
    "outputId": "24dfd72a-915b-4d26-f00e-f6b480e83554"
   },
   "outputs": [],
   "source": [
    "acc = np.average(np.argmax(model.predict(X_test),axis=1)==np.argmax(Y_test,axis=1))\n",
    "res4 = pd.DataFrame(\n",
    "          {'Acc' : acc}, index=['cnn from scratch with dropout']\n",
    ")\n",
    "pd.concat([res1,res2,res3,res4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qP_oH_fcih0D"
   },
   "source": [
    "### CNN from scratch with Batchnorm\n",
    "In this section you train a cnn from scratch to learn to classify the images into the right label. This time you will use batchnorm on the input and in the convolutional part of the network. Note that we use the original images and do not normalize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RZCaUy0PkTL7",
    "outputId": "705a7ee9-96ca-4b7b-f9da-d0c5663124b3"
   },
   "outputs": [],
   "source": [
    "# not normalized, values between 0 and 255\n",
    "X_train[0,0:10,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 988
    },
    "colab_type": "code",
    "id": "fLQ-RZgxih0J",
    "outputId": "d2042963-5e17-4a72-ec00-cf23aae57be4"
   },
   "outputs": [],
   "source": [
    "model  =  Sequential()\n",
    "\n",
    "model.add(BatchNormalization(input_shape=(32,32,3)))\n",
    "model.add(Convolution2D(16,(3,3),padding=\"same\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(16,(3,3),padding=\"same\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "\n",
    "model.add(Convolution2D(32,(3,3),padding=\"same\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(32,(3,3),padding=\"same\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(300))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(100))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "colab_type": "code",
    "id": "_f5HV12Pih0Q",
    "outputId": "b2e46ca9-3d72-452b-ca5c-5c35686d0c9c"
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train, \n",
    "                    batch_size=64,\n",
    "                    epochs=10, validation_data=(X_val, Y_val),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "colab_type": "code",
    "id": "Ar8YM_44ih0Y",
    "outputId": "1010c537-1b37-4853-b72b-9fbe27d7c286"
   },
   "outputs": [],
   "source": [
    "# plot the development of the accuracy and loss during training\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,(1))\n",
    "plt.plot(history.history['accuracy'],linestyle='-.')\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='lower right')\n",
    "plt.subplot(1,2,(2))\n",
    "plt.plot(history.history['loss'],linestyle='-.')\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 172
    },
    "colab_type": "code",
    "id": "jgqm4Erdih0e",
    "outputId": "eabf1fa1-f762-4d89-9c1c-d446f53a0934"
   },
   "outputs": [],
   "source": [
    "acc = np.average(np.argmax(model.predict(X_test),axis=1)==np.argmax(Y_test,axis=1))\n",
    "res5 = pd.DataFrame(\n",
    "          {'Acc' : acc}, index=['cnn from scratch with batchnorm']\n",
    ")\n",
    "pd.concat([res1,res2,res3,res4,res5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y12h3tb5g4r4"
   },
   "source": [
    "#### Exercise\n",
    "Calculate the confusion matrix of the networks.  \n",
    "Play around with the dropout rate and the position of the batchnorm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XiKVO-uOlMbg"
   },
   "source": [
    "### CNN from scratch with Data Augmentation\n",
    "In this section you train a cnn from scratch to learn to classify the images into the right label. This time you will use data augmentation, so the network will train on slightly different versions of the images in each epoch.\n",
    "\n",
    "Data Augmentation is especially helpful if you do not have lots of data. Another approach to train with few data is to use a pretrained neural network. This will be covered in the next notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "colab_type": "code",
    "id": "WXFfkFM5lMbn",
    "outputId": "00f15502-ef7b-40ad-f7d3-b5092e33693b"
   },
   "outputs": [],
   "source": [
    "model  =  Sequential()\n",
    "\n",
    "model.add(Convolution2D(16,(3,3),activation=\"relu\",padding=\"same\",input_shape=(32,32,3)))\n",
    "model.add(Convolution2D(16,(3,3),activation=\"relu\",padding=\"same\"))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "\n",
    "model.add(Convolution2D(32,(3,3),activation=\"relu\",padding=\"same\"))\n",
    "model.add(Convolution2D(32,(3,3),activation=\"relu\",padding=\"same\"))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(300))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(100))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nWzxK6id8FPp"
   },
   "outputs": [],
   "source": [
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.15,\n",
    "    height_shift_range=0.15,\n",
    "    shear_range=0.15,\n",
    "    zoom_range=0.15,\n",
    "fill_mode=\"constant\",\n",
    "cval=1,horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "cQD2VP7s8FMw",
    "outputId": "56ccbb02-f413-4836-d906-6c182751a7a0"
   },
   "outputs": [],
   "source": [
    "i=22\n",
    "data_aug=datagen.flow(x=X_train[i:(i+1)], y=Y_train[i:(i+1)], batch_size=1)\n",
    "plt.imshow(X_train[i])\n",
    "plt.show()\n",
    "plt.figure(figsize=(15,15))\n",
    "for i in range (0,25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    x_aug,y_aug=next(data_aug)\n",
    "    plt.imshow(x_aug[0].astype(\"int\"), vmin = 0, vmax = 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "dgSM-3XT8FE1",
    "outputId": "7a2763e4-2e6b-4b06-e5d2-4d5386ef881a"
   },
   "outputs": [],
   "source": [
    "history = model.fit_generator(datagen.flow(X_train, Y_train, batch_size=64), \n",
    "                              steps_per_epoch=len(X_train)/64, \n",
    "                              epochs=40, \n",
    "                              validation_data=(X_val, Y_val),\n",
    "                              verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "colab_type": "code",
    "id": "19jPWWNeDBgR",
    "outputId": "9dd4628f-d2a9-415a-faaf-51137a2d862d"
   },
   "outputs": [],
   "source": [
    "# plot the development of the accuracy and loss during training\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,(1))\n",
    "plt.plot(history.history['accuracy'],linestyle='-.')\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='lower right')\n",
    "plt.subplot(1,2,(2))\n",
    "plt.plot(history.history['loss'],linestyle='-.')\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "colab_type": "code",
    "id": "IUiqoT9gDafA",
    "outputId": "8673a6a0-f853-4322-985d-b058bebb7995"
   },
   "outputs": [],
   "source": [
    "acc = np.average(np.argmax(model.predict(X_test),axis=1)==np.argmax(Y_test,axis=1))\n",
    "res6 = pd.DataFrame(\n",
    "          {'Acc' : acc}, index=['cnn from scratch with data augmentation']\n",
    ")\n",
    "pd.concat([res1,res2,res3,res4,res5,res6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fWIwUCktrnE4"
   },
   "source": [
    "#### Exercise\n",
    "Try to beat the performace of the best network with your own neutal network.  \n",
    "*Hint: You might want to combine things from the neural networks above*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QoTLI7sErojX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "07_cifar10_tricks",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
