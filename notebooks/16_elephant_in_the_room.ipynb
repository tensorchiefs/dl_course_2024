{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_382Cp9e_5FN"
   },
   "source": [
    "# Using a pretrained Imagenet network to predict images into one of the 1000 Imagenet classes\n",
    "\n",
    "In this notebook you will learn how load a pretrained Imagenet network. You will see how you can download iamges from the web and resize it to the corresponding input size of the network. In addition, you will use the orginal preprocessing of the VGG16 network. Then you will classify some images into one of 1000 classes. Fist you will have a look at rather clear and obvious examples for a dog (affenpinscher) and an elephant (tusker) and then a quite unusual image of an elephant inside a building from the Smithsonian Museum of Natural History in Washington DC. The pretrained network never saw an elephant in that way because it was not part of the Imagenet training dataset. Will the VGG16 network be able to predict the unusual image into the correct class?\n",
    "\n",
    "\n",
    "\n",
    "**Content:**\n",
    "* Load the pretrained VGG16 network that was trained on the 1000 classes of Imagenet\n",
    "* Download and resize images from urls\n",
    "* Define a function to apply the original preprocessing that the VGG team used when they trained the network\n",
    "* Define a function to undo the original preprocessing (to be able to plot the image afterwards, if necessary)\n",
    "* Predict the two clear examples of a dog and an elephant and decode the predictions into the corresponding lables\n",
    "* Predict the unusual image of the elephant in the museum and decode the predictions into the corresponding lables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tqg3DtxjaNKr"
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow==2.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2D6GvXYPG3ID"
   },
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hfvmZgeVaRVu"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('default')\n",
    "\n",
    "print(\"TF  Version\",tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xuVkV9xeG6H-"
   },
   "source": [
    "### Loading the pretrained VGG16 network, trained on the large Imagetnet dataset \n",
    "In the next cells you download the pretrained VGG16 network, you specify inclide_top = True, because you will use the network for classification and not for feature extraction. The weights = \"imagenet\" means that you want to use the pretrained weights and not random weights. When you print the model summary, you can see that the network input size is 224x224x3, so you will need to resize the images into that size. The output is 1000 which corresponds to the probabilities for the 1000 classes, in between we have convolution, maxpooling and dense layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7_Lbm7znaRbP"
   },
   "outputs": [],
   "source": [
    "# The pretrained VGG16 network need quite some memory, \n",
    "# make sure you have enough memory allocated for docker if you are running this notebook locally\n",
    "\n",
    "model_vgg=tf.keras.applications.vgg16.VGG16(include_top=True, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kv2VA8MIaRhT"
   },
   "outputs": [],
   "source": [
    "model_vgg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pd9GxKxpJokZ"
   },
   "source": [
    "In the next cell you define two function to preprocess the input image and to undo the preprocessing. The preprocessing is very simple, it is just substracting the mean value of every channel, the mean values for the channels are calculated on the Imagenet training dataset. Note that we first need to shift the channels around because the VGG team used the BGR and not the RGB format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2d0vv8fmH530"
   },
   "outputs": [],
   "source": [
    "def preprocess_input(img):\n",
    "    x=np.zeros((224,224,3),dtype=\"float32\")\n",
    "    x[:,:,0]=img[:,:,2]\n",
    "    x[:,:,1]=img[:,:,1]\n",
    "    x[:,:,2]=img[:,:,0]\n",
    "    mean = [103.939, 116.779, 123.68]\n",
    "    x[:,:, 0] = x[:,:, 0]-mean[0]\n",
    "    x[:,:, 1] = x[:,:, 1]-mean[1]\n",
    "    x[:,:, 2] = x[:,:, 2]-mean[2]\n",
    "    return x \n",
    "\n",
    "def undo_preprocess_input(img):\n",
    "    mean = [103.939, 116.779, 123.68]\n",
    "    img[:,:, 0] = img[:,:, 0]+mean[0]\n",
    "    img[:,:, 1] = img[:,:, 1]+mean[1]\n",
    "    img[:,:, 2] = img[:,:, 2]+mean[2]\n",
    "    x=np.zeros((224,224,3),dtype=\"float32\")\n",
    "    x[:,:,0]=img[:,:,2]\n",
    "    x[:,:,1]=img[:,:,1]\n",
    "    x[:,:,2]=img[:,:,0]\n",
    "    return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AKp3nKxOKuwf"
   },
   "source": [
    "## Loading two clear images of a dog (affenpinscher) and an elephant (tusker)\n",
    "\n",
    "In the next few cells you will download two images from urls and resize them to the input size of the pretrained VGG16 model which is 224x244x3. You plot them before and after the resizing. As you can see the images are very clear and there should be no problem to classify them into the correct label.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gWKP0NlYaRY9"
   },
   "outputs": [],
   "source": [
    "img1 = (Image.open(urlopen(\"https://raw.githubusercontent.com/tensorchiefs/dl_book/master/imgs/Affenpinscher_Molly.jpg\")))\n",
    "plt.imshow(img1)\n",
    "plt.show()\n",
    "new_width  = 224\n",
    "new_height = 224\n",
    "img1 = img1.resize((new_width, new_height), Image.ANTIALIAS)\n",
    "plt.imshow(img1)\n",
    "plt.show()\n",
    "img1=np.array(img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tF6TfO5nJBBM"
   },
   "outputs": [],
   "source": [
    "img2 = (Image.open(urlopen(\"https://raw.githubusercontent.com/tensorchiefs/dl_book/master/imgs/African_Elephant.jpg\")))\n",
    "plt.imshow(img2)\n",
    "plt.show()\n",
    "new_width  = 224\n",
    "new_height = 224\n",
    "img2 = img2.resize((new_width, new_height), Image.ANTIALIAS)\n",
    "plt.imshow(img2)\n",
    "plt.show()\n",
    "img2=np.array(img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "36jjUtpGJTzx"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img1)\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(img2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKoIAb6TMOeP"
   },
   "source": [
    "Now that the images are in the right size, let's use the network to predict the label. Don't forget to preprocess the input image before the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uEADbjYJdfD7"
   },
   "outputs": [],
   "source": [
    "img1=preprocess_input(img1)\n",
    "print(img1.shape)\n",
    "img2=preprocess_input(img2)\n",
    "print(img2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rxLSAbNBdyQ9"
   },
   "outputs": [],
   "source": [
    "pred1=model_vgg.predict(np.expand_dims(img1,axis=0))\n",
    "tf.keras.applications.vgg16.decode_predictions(pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gqyD2IbfK6py"
   },
   "outputs": [],
   "source": [
    "pred2=model_vgg.predict(np.expand_dims(img2,axis=0))\n",
    "tf.keras.applications.vgg16.decode_predictions(pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "naNMeXW4NMDu"
   },
   "source": [
    "As you can see the network has no problem to predict the correct label, affenpinscher and tusker are there with a high probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZtxyozwLGdj"
   },
   "source": [
    "## Loading and predicting \"the elephant in the room\" \n",
    "\n",
    "Let's see if the VGG16 network is also able to predict an image that was not part of the training dataset, in this case an elephant inside a museum. Note that there are a lot of other objects in the image and the lighting is also not very good. Let's load, resize, preprocess and predict the image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ravlwI1MLb9b"
   },
   "outputs": [],
   "source": [
    "### Image by mana5280 on Unsplash, Smithsonian Museum of Natural History, Washington DC\n",
    "\n",
    "img = (Image.open(urlopen(\"https://raw.githubusercontent.com/tensorchiefs/dl_book/master/imgs/mana5280-o69yU0jE0Nk-unsplash.jpg\")))\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "new_width  = 224\n",
    "new_height = 224\n",
    "img = img.resize((new_width, new_height), Image.ANTIALIAS)\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "img=np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sU41pvX5LcGc"
   },
   "outputs": [],
   "source": [
    "img=preprocess_input(img)\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NJG8zG1ELcJP"
   },
   "outputs": [],
   "source": [
    "pred=model_vgg.predict(np.expand_dims(img,axis=0))\n",
    "tf.keras.applications.vgg16.decode_predictions(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xx2e42CkQtC8"
   },
   "source": [
    "**You can see that the VGG16 network is not able to predict the elephant in the room (the top prediction is horse cart), even though as a human you have no problem at all to see the elephant! The problem is that this is a quite unusual image and in the Imagenet training dataset there were no elephants inside. They used \"normal\" images of elephants in free wilderness.**\n",
    "\n",
    "**This is a principle weakness of deep learning and machine learning in general. We, as humans, obviously learn differently. No child in the world would not see the elephant in this image once she learned what an elephant looks like.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XT9m7iku-fxv"
   },
   "source": [
    "#### Optional Exercise:\n",
    "Read in your own image of an animal in a normal or unusual environment and check the predictions.  \n",
    "Can you find an other \"elephant in the room\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AP0YKI7WR9bH"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from IPython.display import Image, display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "#model_vgg = tf.keras.applications.vgg16.VGG16(include_top=True, weights='imagenet')\n",
    "img_size = (224, 224)\n",
    "preprocess_input = keras.applications.vgg16.preprocess_input\n",
    "decode_predictions = keras.applications.vgg16.decode_predictions\n",
    "last_conv_layer_name = \"block5_conv3\"\n",
    "img_path = keras.utils.get_file(\"tusker.jpg\", \"https://raw.githubusercontent.com/tensorchiefs/dl_book/master/imgs/mana5280-o69yU0jE0Nk-unsplash.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_6bnWtOVYr22"
   },
   "outputs": [],
   "source": [
    "def get_img_array(img_path, size):\n",
    "    img = keras.preprocessing.image.load_img(img_path, target_size=size)    # `img` is a PIL image \n",
    "    array = keras.preprocessing.image.img_to_array(img)    # `array` is a float32 Numpy array of shape (224, 224, 3)\n",
    "    array = np.expand_dims(array, axis=0)    # We add a dimension to transform our array into a \"batch\" of size (1, 224, 224, 3)\n",
    "    return array\n",
    "\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    # First, we create a model that maps the input image to the activations of the last conv layer as well as the output predictions\n",
    "    grad_model = tf.keras.models.Model([model.inputs], [model.get_layer(last_conv_layer_name).output, model.output])\n",
    "    # Then, we compute the gradient of the top predicted class for our input image with respect to the activations of the last conv layer\n",
    "    with tf.GradientTape() as tape:\n",
    "        last_conv_layer_output, preds = grad_model(img_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(preds[0])\n",
    "        class_channel = preds[:, pred_index]\n",
    "    # This is the gradient of the output neuron (top predicted or chosen) with regard to the output feature map of the last conv layer\n",
    "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "    # This is a vector where each entry is the mean intensity of the gradient over a specific feature map channel\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "    # We multiply each channel in the feature map array by \"how important this channel is\" with regard to the top predicted class\n",
    "    last_conv_layer_output = last_conv_layer_output[0]\n",
    "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)     # then sum all the channels to obtain the heatmap class activation\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)     # For visualization purpose, we will also normalize the heatmap between 0 & 1\n",
    "    return heatmap.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8eJKzt5yYuXz"
   },
   "outputs": [],
   "source": [
    "#img_array = preprocess_input(get_img_array(img_path, size=img_size)) # Prepare image\n",
    "img_array = preprocess_input(img).reshape(1,224,224,3)\n",
    "model_vgg.layers[-1].activation = None # Remove last layer's softmax\n",
    "preds = model_vgg.predict(img_array) # Print what the top predicted class is\n",
    "print(\"Predicted:\", decode_predictions(preds, top=5)[0]) \n",
    "heatmap = make_gradcam_heatmap(img_array, model_vgg, last_conv_layer_name)# Generate class activation heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cpemvE_VYxcu"
   },
   "outputs": [],
   "source": [
    "def save_and_display_gradcam(img_path, heatmap, cam_path=\"cam.jpg\", alpha=0.4):\n",
    "    img = keras.preprocessing.image.load_img(img_path)    # Load the original image\n",
    "    img = keras.preprocessing.image.img_to_array(img)\n",
    "    heatmap = np.uint8(255 * heatmap)    # Rescale heatmap to a range 0-255\n",
    "    jet = cm.get_cmap(\"jet\")    # Use jet colormap to colorize heatmap\n",
    "    jet_colors = jet(np.arange(256))[:, :3]    # Use RGB values of the colormap\n",
    "    jet_heatmap = jet_colors[heatmap]    # Create an image with RGB colorized heatmap\n",
    "    jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\n",
    "    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
    "    jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n",
    "    superimposed_img = jet_heatmap * alpha + img    # Superimpose the heatmap on original image\n",
    "    superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n",
    "    superimposed_img.save(cam_path)    # Save the superimposed image\n",
    "    display(Image(cam_path))    # Display Grad CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PkamqgSlmYaL"
   },
   "outputs": [],
   "source": [
    "heatmap = make_gradcam_heatmap(img_array, model_vgg, last_conv_layer_name, pred_index=603) # horsecart\n",
    "save_and_display_gradcam(img_path, heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8cS-tsBGhAC6"
   },
   "outputs": [],
   "source": [
    "heatmap = make_gradcam_heatmap(img_array, model_vgg, last_conv_layer_name, pred_index=698) # index for palace\n",
    "save_and_display_gradcam(img_path, heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "adrxX0avctjX"
   },
   "outputs": [],
   "source": [
    "heatmap = make_gradcam_heatmap(img_array, model_vgg, last_conv_layer_name, pred_index=101) # index for tusker:101,ind:385, afr:386\n",
    "save_and_display_gradcam(img_path, heatmap)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "16_elephant in the room.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "nteract": {
   "version": "0.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
